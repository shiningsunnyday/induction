{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"config\"] = \"/homes/msun415/Documents/GitHub/induction/src/config/ckt.yaml\"\n",
    "os.chdir('/homes/msun415/Documents/GitHub/induction/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from scipy.spatial.distance import pdist\n",
    "import scipy.stats as sps\n",
    "from scipy.stats import pearsonr\n",
    "import hashlib\n",
    "# import gpflow\n",
    "# from gpflow.models import SVGP\n",
    "# from gpflow.optimizers import NaturalGradient\n",
    "# import tensorflow as tf\n",
    "# torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data as GraphData\n",
    "import torch.nn.functional as F\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import pickle\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from src.model import *\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append('dagnn/dvae/bayesian_optimization')\n",
    "from sparse_gp import SparseGP\n",
    "sys.path.append('CktGNN')\n",
    "from utils import is_valid_DAG, is_valid_Circuit\n",
    "\n",
    "# Logging\n",
    "logger = create_logger(\"train\", \"cache/api_ckt_ednce/train.log\")\n",
    "\n",
    "# Generate a random vocabulary of small graphs using NetworkX\n",
    "def generate_random_graphs(vocab_size):\n",
    "    graphs = []\n",
    "    for _ in range(vocab_size):\n",
    "        num_nodes = np.random.randint(3, 6)  # Random number of nodes in each graph token\n",
    "        G = nx.newman_watts_strogatz_graph(num_nodes, 3, p=0.5)\n",
    "        for (u, v) in G.edges():\n",
    "            G.edges[u, v]['weight'] = np.random.rand()  # Random edge weights\n",
    "        graphs.append(G)\n",
    "    return graphs\n",
    "\n",
    "def hash_object(obj):\n",
    "    \"\"\"Create a deterministic hash for a Python object.\"\"\"\n",
    "    obj_bytes = pickle.dumps(obj)\n",
    "    return hashlib.sha256(obj_bytes).hexdigest()\n",
    "\n",
    "def top_sort(edge_index, graph_size):\n",
    "\n",
    "    node_ids = np.arange(graph_size, dtype=int)\n",
    "\n",
    "    node_order = np.zeros(graph_size, dtype=int)\n",
    "    unevaluated_nodes = np.ones(graph_size, dtype=bool)\n",
    "\n",
    "    parent_nodes = edge_index[0].numpy()\n",
    "    child_nodes = edge_index[1].numpy()\n",
    "\n",
    "    n = 0\n",
    "    while unevaluated_nodes.any():\n",
    "        # Find which parent nodes have not been evaluated\n",
    "        unevaluated_mask = unevaluated_nodes[parent_nodes]\n",
    "        \n",
    "        # Find the child nodes of unevaluated parents\n",
    "        unready_children = child_nodes[unevaluated_mask]\n",
    "\n",
    "        # Mark nodes that have not yet been evaluated\n",
    "        # and which are not in the list of children with unevaluated parent nodes\n",
    "        nodes_to_evaluate = unevaluated_nodes & ~np.isin(node_ids, unready_children)\n",
    "\n",
    "        node_order[nodes_to_evaluate] = n\n",
    "        unevaluated_nodes[nodes_to_evaluate] = False\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    return torch.from_numpy(node_order).long()\n",
    "\n",
    "\n",
    "def assert_order(edge_index, o, ns):\n",
    "    # already processed\n",
    "    proc = []\n",
    "    for i in range(max(o)+1):\n",
    "        # nodes in position i in order\n",
    "        l = o == i\n",
    "        l = ns[l].tolist()\n",
    "        for n in l:\n",
    "            # predecessors\n",
    "            ps = edge_index[0][edge_index[1] == n].tolist()\n",
    "            for p in ps:\n",
    "                assert p in proc\n",
    "        proc += l    \n",
    "\n",
    "\n",
    "# to be able to use pyg's batch split everything into 1-dim tensors\n",
    "def add_order_info_01(graph):\n",
    "\n",
    "    l0 = top_sort(graph.edge_index, graph.num_nodes)\n",
    "    ei2 = torch.LongTensor([list(graph.edge_index[1]), list(graph.edge_index[0])])\n",
    "    l1 = top_sort(ei2, graph.num_nodes)\n",
    "    ns = torch.LongTensor([i for i in range(graph.num_nodes)])\n",
    "    graph.__setattr__(\"_bi_layer_idx0\", l0)\n",
    "    graph.__setattr__(\"_bi_layer_index0\", ns)\n",
    "    graph.__setattr__(\"_bi_layer_idx1\", l1)\n",
    "    graph.__setattr__(\"_bi_layer_index1\", ns)\n",
    "\n",
    "    assert_order(graph.edge_index, l0, ns)\n",
    "    assert_order(ei2, l1, ns)    \n",
    "\n",
    "def to_one_hot(y, labels):\n",
    "    one_hot_vector = torch.zeros((len(labels),))\n",
    "    one_hot_vector[labels.index(y)] = 1.\n",
    "    return one_hot_vector\n",
    "\n",
    "\n",
    "# Convert NetworkX graphs to PyTorch Geometric Data objects\n",
    "def convert_graph_to_data(graph):\n",
    "    # Random node features\n",
    "    graph = nx.relabel_nodes(graph, dict(zip(graph,range(len(graph)))))\n",
    "    features = []\n",
    "    term = True\n",
    "    for i in range(len(graph)):           \n",
    "        one_hot_vector = to_one_hot(graph.nodes[i]['label'], TERMS+NONTERMS)\n",
    "        if 'feat' in graph.nodes[i]:\n",
    "            feat_val = graph.nodes[i]['feat']\n",
    "        else:\n",
    "            feat_val = 0.\n",
    "        if one_hot_vector.argmax().item() >= len(TERMS):\n",
    "            term = False # nonterm node\n",
    "        feat = torch.cat((one_hot_vector, torch.tensor([feat_val])))\n",
    "        features.append(feat)\n",
    "    x = torch.stack(features, dim=0)\n",
    "    # x = torch.rand((graph.number_of_nodes(), EMBED_DIM))    \n",
    "    edge_list = list(graph.edges)\n",
    "    edge_index = torch.tensor(edge_list).t().contiguous()\n",
    "    roots = np.setdiff1d(np.arange(len(graph)), edge_index[1])    \n",
    "    dists = [nx.single_source_shortest_path_length(graph, root) for root in roots]\n",
    "    dist = {d: min([dis[d] for dis in dists if d in dis]) for d in range(len(graph))}\n",
    "    node_depth = [dist[i] for i in range(len(graph))]\n",
    "    edge_attr = torch.stack([to_one_hot(graph.edges[edge]['label'], FINAL+NONFINAL) for edge in edge_list], dim=0)\n",
    "    g = GraphData(x=x, edge_index=edge_index, node_depth=torch.tensor(node_depth), edge_attr=edge_attr)\n",
    "    add_order_info_01(g) # dagnn\n",
    "    g.len_longest_path = float(torch.max(g._bi_layer_idx0).item()) # dagnn\n",
    "    return g, term\n",
    "\n",
    "def add_ins(g, ins):\n",
    "    if ins is None:\n",
    "        return g\n",
    "    for mu, p, q, i, d, d_ in ins:        \n",
    "        n = len(g)\n",
    "        g.add_node(n, label=mu)\n",
    "        if d_ == 'out':\n",
    "            g.add_edge(list(g)[i], n, label=q)        \n",
    "        else:\n",
    "            g.add_edge(n, list(g)[i], label=q)\n",
    "    return g\n",
    "\n",
    "# Prepare the graph vocabulary\n",
    "graph_vocabulary = None # generate_random_graphs(VOCAB_SIZE)\n",
    "graph_data_vocabulary = None\n",
    "vocabulary_terminate = None\n",
    "\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = []\n",
    "        self.data = data\n",
    "        for idx in range(len(data)):\n",
    "            seq = []\n",
    "            graph_seq = {}\n",
    "            for i in range(len(self.data[idx])):\n",
    "                r, g, ins = self.data[idx][i]\n",
    "                g = add_ins(g, ins)\n",
    "                seq.append(r)\n",
    "                graph, _ = convert_graph_to_data(g)\n",
    "                graph_seq[i] = graph                  \n",
    "            self.dataset.append((torch.tensor(seq), graph_seq, idx))\n",
    "        self.perm = np.arange(len(data))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def shuffle(self):\n",
    "        self.perm = np.random.permutation(self.perm)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.perm[idx]]\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = []\n",
    "        self.data = data\n",
    "        for idx in range(len(data)):\n",
    "            seq, graph = self.data[idx]\n",
    "            graph, _ = convert_graph_to_data(graph)\n",
    "            self.dataset.append((torch.tensor(seq), graph, idx))\n",
    "        self.perm = np.arange(len(data))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.perm = np.random.permutation(self.perm)        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.perm[idx]]\n",
    "\n",
    "\n",
    "# Define GNN-based Token Embedding\n",
    "class TokenGNN(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(TokenGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(embed_dim, embed_dim)\n",
    "        self.conv2 = GCNConv(embed_dim, embed_dim)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, graph_data):\n",
    "        x, edge_index = graph_data.x, graph_data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pooling(x.t()).squeeze(-1)  # Aggregate node features to a single vector\n",
    "        return x\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def vae_loss(args, recon_logits, mask, x, mu, logvar):    \n",
    "    x_flat = x.view(-1)\n",
    "    recon_loss = F.cross_entropy(recon_logits, x_flat, reduction=\"none\")\n",
    "    recon_loss = recon_loss.view(x.size(0), -1)\n",
    "    recon_loss = (recon_loss * mask).sum() / mask.sum()\n",
    "    kl_divergence = -args.klcoeff * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_divergence / x.size(0)\n",
    "\n",
    "\n",
    "# Padding function\n",
    "def collate_batch(batch):    \n",
    "    lengths = [len(seq) for seq, _, _ in batch]\n",
    "    max_len = max(lengths)\n",
    "    padded_batch = torch.zeros(len(batch), max_len, dtype=torch.long)    \n",
    "    seq_len_list = torch.zeros(len(batch), dtype=torch.long)\n",
    "    attention_mask = torch.zeros((len(batch), max_len), dtype=torch.bool)\n",
    "    graphs = []\n",
    "    idxes = []\n",
    "    batch_g_list = []            \n",
    "    for i, (seq, graph, idx) in enumerate(batch):\n",
    "        padded_batch[i, :len(seq)] = seq\n",
    "        seq_len_list[i] = len(seq)       \n",
    "        idxes.append(idx)\n",
    "        attention_mask[i, :len(seq)] = 1\n",
    "        g_list = [None] * max_len # pad to max_len\n",
    "        # g_list.update({i: None for i in range(len(g_list), max_len)})\n",
    "        batch_g_list.append(g_list)  \n",
    "    return padded_batch, attention_mask, seq_len_list, batch_g_list, idxes \n",
    "    \n",
    "\n",
    "def collate_batch_gnn(batch):\n",
    "    lengths = [len(seq) for seq, _, _ in batch]\n",
    "    max_len = max(lengths)\n",
    "    padded_batch = torch.zeros(len(batch), max_len, dtype=torch.long)    \n",
    "    seq_len_list = torch.zeros(len(batch), dtype=torch.long)\n",
    "    attention_mask = torch.zeros((len(batch), max_len), dtype=torch.bool)\n",
    "    graphs = []\n",
    "    idxes = []          \n",
    "    for i, (seq, graph, idx) in enumerate(batch):\n",
    "        padded_batch[i, :len(seq)] = seq\n",
    "        seq_len_list[i] = len(seq)       \n",
    "        idxes.append(idx)\n",
    "        attention_mask[i, :len(seq)] = 1\n",
    "        graphs.append(graph)         \n",
    "    return padded_batch, attention_mask, seq_len_list, graphs, idxes   \n",
    "\n",
    "\n",
    "\n",
    "# Sampling new sequences\n",
    "def sample(model, num_samples=5, max_seq_len=10):\n",
    "    model.eval()\n",
    "    uniq_sequences = set()\n",
    "    with torch.no_grad():\n",
    "        while len(uniq_sequences) < num_samples:\n",
    "            z = torch.randn(num_samples, args.latent_dim)  # Sample from the prior\n",
    "            z = z.to(args.cuda)\n",
    "            generated_sequences = model.autoregressive_inference(z, token2rule, max_seq_len)  # Decode from latent space            \n",
    "            uniq_sequences = uniq_sequences | set(map(tuple, generated_sequences))\n",
    "    uniq_sequences = [list(l) for l in uniq_sequences]\n",
    "    return uniq_sequences[:num_samples]\n",
    "\n",
    "\n",
    "def decode_from_latent_space(z, grammar, model, token2rule, max_seq_len):\n",
    "    generated_sequences = model.autoregressive_inference(z, max_seq_len)\n",
    "    generated_dags = []\n",
    "    for deriv in generated_sequences:\n",
    "        g = grammar.derive(deriv, token2rule)\n",
    "        generated_dags.append(g)\n",
    "    return generated_dags\n",
    "    \n",
    "\n",
    "def train(args, train_data, test_data):\n",
    "    folder = args.datapkl if args.datapkl else args.folder\n",
    "    # Initialize model and optimizer\n",
    "    model = TransformerVAE(args.encoder, args.encoder_layers, args.decoder_layers, VOCAB_SIZE, vocabulary_init, vocabulary_terminate, args.embed_dim, args.latent_dim, MAX_SEQ_LEN)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    if args.encoder == \"GNN\":\n",
    "        train_dataset = GraphDataset(train_data)\n",
    "        test_dataset = GraphDataset(test_data) \n",
    "    else:       \n",
    "        # Dummy dataset: Replace with actual sequence data\n",
    "        # dataset = [torch.tensor(seq) for seq in data]\n",
    "        train_dataset = TokenDataset(train_data)\n",
    "        # train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, num_workers=8, timeout=1000, prefetch_factor=1)\n",
    "        test_dataset = TokenDataset(test_data)\n",
    "        # test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch, num_workers=8, timeout=1000, prefetch_factor=1)    \n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    model = model.to(args.cuda)\n",
    "    if args.encoder == \"TOKEN_GNN\":\n",
    "        for i, graph_data in enumerate(graph_data_vocabulary):\n",
    "            graph_data_vocabulary[i] = graph_data.to(args.cuda)\n",
    "    ckpts = glob.glob(f'ckpts/api_ckt_ednce/{folder}/*.pth')\n",
    "    start_epoch = 0\n",
    "    best_loss = float(\"inf\")\n",
    "    best_ckpt_path = None\n",
    "    for ckpt in ckpts:\n",
    "        epoch = int(re.findall('epoch=(\\d+)',ckpt)[0])\n",
    "        loss = float(re.findall('loss=([\\d.]+).pth',ckpt)[0])\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            start_epoch = epoch + 1\n",
    "            best_ckpt_path = ckpt\n",
    "    logger.info(best_ckpt_path)    \n",
    "    if best_ckpt_path is not None:\n",
    "        logger.info(f\"loaded {best_ckpt_path} loss {best_loss} start_epoch {start_epoch}\")\n",
    "        model.load_state_dict(torch.load(best_ckpt_path, map_location=args.cuda))\n",
    "\n",
    "    patience = 25\n",
    "    patience_counter = 0\n",
    "    train_latent = np.empty((len(train_data), args.latent_dim))\n",
    "    test_latent = np.empty((len(test_data), args.latent_dim))\n",
    "    for epoch in tqdm(range(start_epoch, args.epochs+1)):\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        rec_acc_sum = 0.\n",
    "        train_dataset.shuffle()\n",
    "        g_batch = []\n",
    "        for i in tqdm(range(len(train_dataset))):\n",
    "            g_batch.append(train_dataset[i])\n",
    "            if len(g_batch) == args.batch_size or i == len(train_dataset)-1:\n",
    "                if args.encoder == \"GNN\":\n",
    "                    batch = collate_batch_gnn(g_batch)\n",
    "                else:\n",
    "                    batch = collate_batch(g_batch)\n",
    "                x, attention_mask, seq_len_list, batch_g_list, batch_idxes = batch\n",
    "                x, attention_mask = x.to(args.cuda), attention_mask.to(args.cuda)\n",
    "                optimizer.zero_grad()\n",
    "                recon_logits, mask, mu, logvar = model(x, attention_mask, seq_len_list, batch_g_list)                           \n",
    "                loss = vae_loss(args, recon_logits, mask, x, mu, logvar)\n",
    "                loss.backward()            \n",
    "                train_loss += loss.item()*len(batch_idxes)\n",
    "                rll = recon_logits.argmax(axis=-1).reshape(x.shape)\n",
    "                rec_acc = (rll == x).all(axis=-1)\n",
    "                rec_acc_sum += rec_acc.sum()\n",
    "                optimizer.step()\n",
    "                train_latent[batch_idxes] = mu.detach().cpu().numpy()\n",
    "                g_batch = []\n",
    "        train_loss /= len(train_dataset)\n",
    "        train_rec_acc_mean = rec_acc_sum / len(train_dataset)\n",
    "        model.eval()\n",
    "        val_loss = 0.\n",
    "        rec_acc_sum = 0.\n",
    "        g_batch = []\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(len(test_dataset))):\n",
    "                g_batch.append(test_dataset[i])\n",
    "                if len(g_batch) == args.batch_size or i == len(test_dataset)-1:\n",
    "                    if args.encoder == \"GNN\":\n",
    "                        batch = collate_batch_gnn(g_batch)\n",
    "                    else:\n",
    "                        batch = collate_batch(g_batch)               \n",
    "                    x, attention_mask, seq_len_list, batch_g_list, batch_idxes = batch\n",
    "                    x, attention_mask = x.to(args.cuda), attention_mask.to(args.cuda)\n",
    "                    recon_logits, mask, mu, logvar = model(x, attention_mask, seq_len_list, batch_g_list)\n",
    "                    loss = vae_loss(args, recon_logits, mask, x, mu, logvar)            \n",
    "                    val_loss += loss.item()*len(batch_idxes)\n",
    "                    rll = recon_logits.argmax(axis=-1).reshape(x.shape)\n",
    "                    rec_acc = (rll == x).all(axis=-1)\n",
    "                    rec_acc_sum += rec_acc.sum()\n",
    "                    test_latent[batch_idxes] = mu.detach().cpu().numpy()\n",
    "                    g_batch = []   \n",
    "        val_loss /= len(test_dataset)\n",
    "        valid_rec_acc_mean = rec_acc_sum / len(test_dataset)\n",
    "        if val_loss < best_loss:\n",
    "            patience_counter = 0 # reset counter\n",
    "            best_loss = val_loss\n",
    "            ckpt_path = f'ckpts/api_ckt_ednce/{args.folder}/epoch={epoch}_loss={best_loss}.pth'\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            logger.info(ckpt_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            logger.info(f\"No improvement from best loss: {best_loss}, patience: {patience_counter}/{patience}\")\n",
    "        logger.info(f\"Run Details:\\n\"\n",
    "            f\"  - Encoder: {args.encoder}\"\n",
    "            f\"  - Embedding Dimension: {args.embed_dim}\"\n",
    "            f\"  - Latent Dimension: {args.latent_dim}\"\n",
    "            f\"  - Encoder Layers: {args.encoder_layers}\"\n",
    "            f\"  - Decoder Layers: {args.decoder_layers}\"\n",
    "            f\"  - Batch Size: {args.batch_size}\"\n",
    "            f\"  - KL Divergence Coefficient: {args.klcoeff}\")\n",
    "        logger.info(f\"Epoch {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}, Train Rec: {train_rec_acc_mean}, Val Rec: {valid_rec_acc_mean}\")\n",
    "        np.save(f'ckpts/api_ckt_ednce/{args.folder}/train_latent_{epoch}.npy', train_latent)\n",
    "        np.save(f'ckpts/api_ckt_ednce/{args.folder}/test_latent_{epoch}.npy', test_latent)\n",
    "        fig = model.visualize_tokens()\n",
    "        fig.savefig(f'ckpts/api_ckt_ednce/{args.folder}/{epoch}.png')        \n",
    "        embedding = model.token_embedding.weight.detach().cpu().numpy()\n",
    "        np.save(f'ckpts/api_ckt_ednce/{args.folder}/embedding_{epoch}.npy', embedding)\n",
    "        if patience_counter > patience:\n",
    "            logger.info(f\"Early stopping triggered at epoch {epoch}. Best loss: {best_loss}\")\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_sgp(sgp, input_means, training_targets, batch_size=1000, lr=1e-4, max_iter=500):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((input_means, training_targets))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    # Define optimizer\n",
    "    adam_optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "    # Training function\n",
    "    @tf.function\n",
    "    def train_step(model, optimizer, batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            data_input, data_output = batch\n",
    "            # Compute variational ELBO loss\n",
    "            loss = -model.elbo((data_input, data_output))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    # Train for a given number of iterations\n",
    "    for i in range(max_iter):\n",
    "        loss_total = 0.\n",
    "        for step, batch in enumerate(train_dataset):\n",
    "            loss = train_step(sgp, adam_optimizer, batch)\n",
    "            loss_total += loss.numpy()\n",
    "        logger.info(f\"Epoch {i}: Loss = {loss_total}\")\n",
    "        sgp.predict(X_test)\n",
    "\n",
    "def bo(args, grammar, model, token2rule, y_train, y_test):\n",
    "    folder = args.datapkl if args.datapkl else args.folder\n",
    "    ckpt_dir = f'ckpts/api_ckt_ednce/{folder}'    \n",
    "    X_train = np.load(os.path.join(ckpt_dir, f\"train_latent_{args.checkpoint}.npy\"))    \n",
    "    X_test = np.load(os.path.join(ckpt_dir, f\"test_latent_{args.checkpoint}.npy\"))    \n",
    "    X_train_mean = X_train.mean(axis=0)\n",
    "    X_train_std = X_train.std(axis=0)\n",
    "    X_train = (X_train-X_train_mean)/X_train_std\n",
    "    X_test = (X_test-X_train_mean)/X_train_std    \n",
    "    iteration = 0\n",
    "    best_score = 1e15\n",
    "    best_arc = None\n",
    "    best_random_score = 1e15\n",
    "    best_random_arc = None\n",
    "    logger.info(\"Average pairwise distance between train points = {}\".format(np.mean(pdist(X_train))))\n",
    "    logger.info(\"Average pairwise distance between test points = {}\".format(np.mean(pdist(X_test))))    \n",
    "    while iteration < args.BO_rounds:\n",
    "        logger.info(f\"Iteration: {iteration}\")\n",
    "        if args.predictor:\n",
    "            pred = model.predictor(torch.FloatTensor(X_test).to(args.cuda))\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            pred = (-pred - mean_y_train) / std_y_train\n",
    "            uncert = np.zeros_like(pred)\n",
    "        else:\n",
    "            # We fit the GP\n",
    "            M = 500\n",
    "            # other BO hyperparameters\n",
    "            lr = 0.005  # the learning rate to train the SGP model\n",
    "            max_iter = args.max_iter  # how many iterations to optimize the SGP each time\n",
    "            sgp = SparseGP(X_train, 0 * X_train, y_train, M)\n",
    "            sgp.train_via_ADAM(X_train, 0 * X_train, y_train, X_test, X_test * 0, y_test, minibatch_size = 2 * M, max_iterations = max_iter, learning_rate = lr)\n",
    "            pred, uncert = sgp.predict(X_test, 0 * X_test)\n",
    "            # input_means = X_train\n",
    "            # input_vars = np.zeros_like(X_train)  # Variances initialized to 0\n",
    "            # training_targets = y_train\n",
    "            # n_inducing_points = M\n",
    "            # # Define kernel\n",
    "            # kernel = gpflow.kernels.RBF()\n",
    "            # # Initialize inducing points (e.g., random subset of training data)\n",
    "            # inducing_points = input_means[:n_inducing_points, :]\n",
    "            # # Create sparse variational GP model\n",
    "            # sgp = SVGP(kernel=kernel,\n",
    "            #         likelihood=gpflow.likelihoods.Gaussian(),\n",
    "            #         inducing_variable=inducing_points,\n",
    "            #         num_latent_gps=1)\n",
    "            # train_sgp(sgp, input_means, training_targets, batch_size=2*M)\n",
    "            # pred, var = sgp.predict_y(X_test)\n",
    "\n",
    "        logger.info(f\"predictions: {pred.reshape(-1)}\")\n",
    "        logger.info(f\"real values: {y_test.reshape(-1)}\")\n",
    "        error = np.sqrt(np.mean((pred - y_test)**2))\n",
    "        testll = np.mean(sps.norm.logpdf(pred - y_test, scale = np.sqrt(uncert)))\n",
    "        logger.info(f'Test RMSE: {error}')\n",
    "        logger.info(f'Test ll: {testll}')\n",
    "        pearson = float(pearsonr(pred.flatten(), y_test.flatten())[0])\n",
    "        logger.info(f'Pearson r: {pearson}')\n",
    "        with open('results/' + 'Test_RMSE_ll.txt', 'a') as test_file:\n",
    "            test_file.write('Test RMSE: {:.4f}, ll: {:.4f}, Pearson r: {:.4f}\\n'.format(error, testll, pearson))\n",
    "\n",
    "        error_if_predict_mean = np.sqrt(np.mean((np.mean(y_train, 0) - y_test)**2))\n",
    "        logger.info(f'Test RMSE if predict mean: {error_if_predict_mean}')\n",
    "        if args.predictor:\n",
    "            pred = model.predictor(torch.FloatTensor(X_train).to(args.cuda))\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            pred = (-pred - mean_y_train) / std_y_train\n",
    "            uncert = np.zeros_like(pred)\n",
    "        else:\n",
    "            pred, uncert = sgp.predict(X_train, 0 * X_train)\n",
    "        error = np.sqrt(np.mean((pred - y_train)**2))\n",
    "        trainll = np.mean(sps.norm.logpdf(pred - y_train, scale = np.sqrt(uncert)))\n",
    "        logger.info(f'Train RMSE: {error}')\n",
    "        logger.info(f'Train ll: {trainll}')        \n",
    "        next_inputs = sgp.batched_greedy_ei(args.BO_batch_size, np.min(X_train, 0), np.max(X_train, 0), np.mean(X_train, 0), np.std(X_train, 0), sample=args.sample_dist, max_iter=args.max_ei_iter)\n",
    "        breakpoint()\n",
    "        valid_arcs_final = decode_from_latent_space(torch.FloatTensor(next_inputs).to(args.cuda), grammar, model, token2rule, MAX_SEQ_LEN)        \n",
    "        new_features = next_inputs\n",
    "        logger.info(\"Evaluating selected points\")\n",
    "        scores = []\n",
    "        for i in range(len(valid_arcs_final)):\n",
    "            score = evaluate(args, valid_arcs_final[i])\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_arc = arc\n",
    "            scores.append(score)\n",
    "            # logger.info(i, score)\n",
    "        # logger.info(\"Iteration {}'s selected arcs' scores:\".format(iteration))\n",
    "        # logger.info(scores, np.mean(scores))\n",
    "        save_object(scores, \"{}scores{}.dat\".format(save_dir, iteration))\n",
    "        save_object(valid_arcs_final, \"{}valid_arcs_final{}.dat\".format(save_dir, iteration))\n",
    "\n",
    "        if len(new_features) > 0:\n",
    "            X_train = np.concatenate([ X_train, new_features ], 0)\n",
    "            y_train = np.concatenate([ y_train, np.array(scores)[ :, None ] ], 0)\n",
    "        #\n",
    "        # logger.info(\"Current iteration {}'s best score: {}\".format(iteration, - best_score * std_y_train - mean_y_train))\n",
    "        if best_arc is not None: # and iteration == 10:\n",
    "            logger.info(f\"Best architecture: {best_arc}\")\n",
    "            with open(save_dir + 'best_arc_scores.txt', 'a') as score_file:\n",
    "                score_file.write(best_arc + ', {:.4f}\\n'.format(-best_score * std_y_train - mean_y_train))\n",
    "            if data_type == 'ENAS':\n",
    "                row = [int(x) for x in best_arc.split()]\n",
    "                g_best, _ = decode_ENAS_to_igraph(flat_ENAS_to_nested(row, max_n-2))\n",
    "            elif data_type == 'BN':\n",
    "                row = adjstr_to_BN(best_arc)\n",
    "                g_best, _ = decode_BN_to_igraph(row)\n",
    "            plot_DAG(g_best, save_dir, 'best_arc_iter_{}'.format(iteration), data_type=data_type, pdf=True)\n",
    "        #\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "def visualize_sequences(sampled_sequences, grammar, token2rule):\n",
    "    logger.info(\"===SAMPLED SEQUENCES===\")\n",
    "    for i, seq in enumerate(sampled_sequences):\n",
    "        logger.info('->'.join(map(str, seq)))\n",
    "        # Visualize new sequence\n",
    "        path = f'data/api_ckt_ednce/generate/{i}.png'\n",
    "        img_path = f'data/api_ckt_ednce/generate/{i}_g.png'\n",
    "        fig, axes = plt.subplots(len(seq), figsize=(5, 5*(len(seq))))\n",
    "        for idx, j in enumerate(map(int, seq)):\n",
    "            r = grammar.rules[j]\n",
    "            draw_graph(r.subgraph, ax=axes[idx], scale=5)\n",
    "        fig.savefig(path)\n",
    "        logger.info(os.path.abspath(path))\n",
    "        g = grammar.derive(seq, token2rule)        \n",
    "        draw_graph(g, path=img_path)    \n",
    "\n",
    "\n",
    "def sample_sequences(model, grammar, token2rule, num_samples=5, max_seq_len=10):\n",
    "    # Generate and logger.info new sequences\n",
    "    sampled_sequences = sample(model, num_samples, max_seq_len)\n",
    "    visualize_sequences(sampled_sequences, grammar, token2rule)\n",
    "\n",
    "\n",
    "\n",
    "def interactive_sample_sequences(args, model, grammar, token2rule, num_samples=5, max_seq_len=10, unique=False, visualize=False):\n",
    "    num_samples = args.num_samples\n",
    "    sample_batch_size = args.sample_batch_size\n",
    "    model.eval()\n",
    "    if unique:\n",
    "        uniq_sequences = set()\n",
    "    else:\n",
    "        sequences = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=num_samples) as pbar:\n",
    "            while len(uniq_sequences if unique else sequences) < num_samples:\n",
    "                z = torch.randn(sample_batch_size, args.latent_dim)  \n",
    "                z = z.to(args.cuda)            \n",
    "                generated_sequences = model.autoregressive_interactive_inference(z, grammar, token2rule, max_seq_len=max_seq_len)    \n",
    "                if unique:\n",
    "                    uniq_sequences = uniq_sequences | set(map(tuple, generated_sequences))                \n",
    "                    pbar.update(len(uniq_sequences)-pbar.n)\n",
    "                else:\n",
    "                    sequences += generated_sequences\n",
    "                    pbar.update(len(sequences)-pbar.n)\n",
    "    if unique:\n",
    "        uniq_sequences = [list(l) for l in uniq_sequences]\n",
    "        sampled_sequences = uniq_sequences[:num_samples]\n",
    "    else:\n",
    "        sampled_sequences = sequences[:num_samples]\n",
    "    logger.info(\"===SAMPLED SEQUENCES===\")\n",
    "    gs = []\n",
    "    for i, seq in enumerate(sampled_sequences):\n",
    "        logger.info('->'.join(map(str, seq)))\n",
    "        # Visualize new sequence\n",
    "        if visualize:\n",
    "            path = f'data/api_ckt_ednce/generate/{i}.png'\n",
    "            img_path = f'data/api_ckt_ednce/generate/{i}_g.png'\n",
    "            fig, axes = plt.subplots(len(seq), figsize=(5, 5*(len(seq))))\n",
    "            for idx, j in enumerate(map(int, seq)):\n",
    "                r = grammar.rules[j]\n",
    "                draw_graph(r.subgraph, ax=axes[idx], scale=5)\n",
    "            fig.savefig(path)\n",
    "            logger.info(os.path.abspath(path))\n",
    "            g = grammar.derive(seq, token2rule)\n",
    "            draw_graph(g, path=img_path)    \n",
    "        else:\n",
    "            g = grammar.derive(seq, token2rule)\n",
    "        gs.append(g)\n",
    "    return gs\n",
    "    \n",
    "\n",
    "def load_y(g, num_graphs):\n",
    "    y = []\n",
    "    for pre in range(num_graphs):        \n",
    "        y.append(g.graph[f'{pre}:fom'])\n",
    "    return y\n",
    "\n",
    "\n",
    "def derive(seq):        \n",
    "    all_applied = []\n",
    "    all_node_maps = []\n",
    "    # find the initial rule\n",
    "    start_rule = seq[0]\n",
    "    cur = deepcopy(start_rule.subgraph)\n",
    "    all_node_maps.append({n:n for n in cur})\n",
    "    assert not check_input_xor_output(cur)\n",
    "    for rule in seq[1:]:\n",
    "        nt_nodes = list(filter(lambda x: cur.nodes[x][\"label\"] in NONTERMS, cur))\n",
    "        if len(nt_nodes) == 0:\n",
    "            break\n",
    "        assert len(nt_nodes) == 1\n",
    "        node = nt_nodes[0]\n",
    "        cur, applied, node_map = rule(cur, node, return_applied=True)\n",
    "        all_applied.append(applied)\n",
    "        all_node_maps.append(node_map)                       \n",
    "    return cur, all_applied, all_node_maps  \n",
    "\n",
    "\n",
    "def process_single(g_orig, rules):\n",
    "    rule_ids = [r[0] for r in rules]\n",
    "    rules = [r[1] for r in rules]\n",
    "    g, all_applied, all_node_maps = derive(rules)\n",
    "    matcher = DiGraphMatcher(g, g_orig, node_match=node_match)\n",
    "    iso = next(matcher.isomorphisms_iter())\n",
    "    # use iso to embed feats and instructions        \n",
    "    for i, r in enumerate(rules):\n",
    "        sub = deepcopy(nx.DiGraph(r.subgraph))\n",
    "        # node feats\n",
    "        for n in sub:\n",
    "            key = all_node_maps[i][n]\n",
    "            if key in iso:\n",
    "                sub.nodes[n]['feat'] = g_orig.nodes[iso[key]]['feat']\n",
    "        rule_ids[i] = (rule_ids[i], sub, all_applied[i-1] if i else None)\n",
    "    return rule_ids\n",
    "\n",
    "\n",
    "def load_data(args, anno, grammar, orig, cache_dir, num_graphs):\n",
    "    loaded = False\n",
    "    if args.datapkl:\n",
    "        save_path = os.path.join(cache_dir, args.datapkl, 'data.pkl')\n",
    "        if os.path.exists(args.datapkl): # specified to load data from args.datapkl path\n",
    "            logger.info(f\"load data from {save_path}\")\n",
    "            data, rule2token = pickle.load(open(save_path, 'rb'))\n",
    "            loaded = True\n",
    "    if not loaded:        \n",
    "        if args.datapkl:\n",
    "            save_path = os.path.join(cache_dir, args.datapkl, 'data.pkl')\n",
    "        else:\n",
    "            save_path = os.path.join(cache_dir, args.folder, 'data.pkl')\n",
    "        # for ckt only, duplicate and interleave anno\n",
    "        logger.info(f\"begin load_data\")\n",
    "        anno_copy = deepcopy(anno)\n",
    "        anno = {}\n",
    "        for n in anno_copy:\n",
    "            p = get_prefix(n)\n",
    "            s = get_suffix(n)\n",
    "            anno[f\"{2*p}:{s}\"] = anno_copy[n]\n",
    "            anno[f\"{2*p+1}:{s}\"] = deepcopy(anno_copy[n])\n",
    "        exist = os.path.exists(save_path)\n",
    "        if exist:\n",
    "            logger.info(f\"load data from {save_path}\")\n",
    "            data, rule2token = pickle.load(open(save_path, 'rb'))\n",
    "        else:\n",
    "            find_anno = {}        \n",
    "            for k in anno:\n",
    "                if get_prefix(k) not in find_anno:\n",
    "                    find_anno[get_prefix(k)] = []\n",
    "                find_anno[get_prefix(k)].append(k)\n",
    "            rule2token = {}\n",
    "            pargs = []\n",
    "            data = []\n",
    "            for pre in tqdm(range(num_graphs), \"gathering rule tokens\"):\n",
    "                seq = find_anno[pre]\n",
    "                seq = seq[::-1] # derivation\n",
    "                rule_ids = [anno[s].attrs['rule'] for s in seq]\n",
    "                # orig_nodes = [list(anno[s].attrs['nodes']) for s in seq]\n",
    "                # orig_feats = [[orig.nodes[n]['feat'] if n in orig else 0.0 for n in nodes] for nodes in orig_nodes]    \n",
    "                # g_orig = copy_graph(orig, orig.comps[pre])\n",
    "                g_orig = nx.induced_subgraph(orig, orig.comps[pre]).copy()                \n",
    "                for i, r in enumerate(rule_ids):\n",
    "                    # from networkx.algorithms.isomorphism import DiGraphMatcher\n",
    "                    rule2token[r] = grammar.rules[r].subgraph\n",
    "                    # matcher = DiGraphMatcher(copy_graph(g, orig_nodes[i]), rule2token[r], node_match=node_match)\n",
    "                    # breakpoint()\n",
    "                    # assert any(all([iso[orig_nodes[i][j]] == list(rule2token[r])[j]]) for iso in matcher.isomorphisms_iter())                \n",
    "                if args.encoder == \"GNN\":\n",
    "                    data.append((rule_ids, g_orig))                \n",
    "                else:                    \n",
    "                    rules = [(r, grammar.rules[r]) for r in rule_ids]\n",
    "                    pargs.append((g_orig, rules))\n",
    "            if args.encoder != \"GNN\":\n",
    "                with mp.Pool(20) as p:\n",
    "                    data = p.starmap(process_single, tqdm(pargs, \"processing data mp\"))\n",
    "            pickle.dump((data, rule2token), open(save_path, 'wb+'))\n",
    "    relabel = dict(zip(list(sorted(rule2token)), range(len(rule2token))))    \n",
    "    token2rule = dict(zip(relabel.values(), relabel.keys()))\n",
    "    if args.encoder == \"GNN\":\n",
    "        data = [([relabel[s] for s in seq], g) for seq, g in data]    \n",
    "    else:\n",
    "        data = [[(relabel[s[0]],)+s[1:] for s in seq] for seq in data]    \n",
    "    globals()['graph_vocabulary'] = list(rule2token.values())\n",
    "    terminate, init = {}, {}\n",
    "    vocab = []\n",
    "    for key, graph in rule2token.items():\n",
    "        graph_data, term = convert_graph_to_data(graph)\n",
    "        terminate[relabel[key]] = term\n",
    "        init[relabel[key]] = grammar.rules[key].nt == 'black'\n",
    "        vocab.append(graph_data)\n",
    "    if args.encoder == \"GNN\":\n",
    "        globals()['MAX_SEQ_LEN'] = max([len(seq) for seq, _ in data])\n",
    "    else:\n",
    "        globals()['MAX_SEQ_LEN'] = max([len(seq) for seq in data])\n",
    "    globals()['graph_data_vocabulary'] = vocab\n",
    "    globals()['vocabulary_terminate'] = terminate\n",
    "    globals()['vocabulary_init'] = init\n",
    "    globals()['VOCAB_SIZE'] = len(rule2token)\n",
    "    # split here\n",
    "    indices = list(range(num_graphs))\n",
    "    # random.Random(0).shuffle(indices)\n",
    "    train_indices, test_indices = indices[:int(num_graphs*0.9)], indices[int(num_graphs*0.9):]\n",
    "    train_data = [data[i] for i in train_indices]\n",
    "    test_data = [data[i] for i in test_indices]\n",
    "    return train_data, test_data, token2rule\n",
    "\n",
    "\n",
    "def hash_args(args, ignore_keys=['datapkl', 'checkpoint']):\n",
    "    arg_dict = {k: v for k, v in args.__dict__.items() if k not in ignore_keys}\n",
    "    return hashlib.md5(json.dumps(arg_dict, sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "def is_novel(g, orig_graphs):\n",
    "    for o in orig_graphs:\n",
    "        if nx.is_isomorphic(g, o, node_match=node_match):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_valid_circuit(g):    \n",
    "    s = next((g.nodes[n]['type'] == 'input'))\n",
    "    t = next((g.nodes[n]['type'] == 'output'))\n",
    "    for path in nx.all_shortest_paths(g, s, t):\n",
    "        for i in path:\n",
    "            if g.nodes[i]['type'] in ['-gm-','+gm-']:                \n",
    "                good = False\n",
    "                predecessors_ = g.predecessors(i)\n",
    "                successors_ = g.successors(i)\n",
    "                for v_p in predecessors_:\n",
    "                    v_p_succ = g.successors(v_p)\n",
    "                    for v_cand in v_p_succ:\n",
    "                        inster_set = set(g.successors(v_cand)) & set(successors_)\n",
    "                        if g.nodes[v_cand]['type'] in ['R','C'] and len(inster_set):\n",
    "                            good = True\n",
    "            if not good:\n",
    "                return False\n",
    "    return True              \n",
    "\n",
    "\n",
    "def evaluate(orig_graphs, graphs):\n",
    "    ckt_valid, dag_valid, novel = [], [], []\n",
    "    for g in graphs:\n",
    "        if not is_valid_circuit(g):\n",
    "            breakpoint()\n",
    "        is_valid_ckt = is_valid_circuit(g)\n",
    "        ckt_valid.append(is_valid_ckt)\n",
    "        dag_valid.append(nx.is_directed_acyclic_graph(g))\n",
    "        novel.append(is_novel(g, orig_graphs))\n",
    "    return {\"valid_dag\": np.mean(dag_valid), \"valid_ckt\": np.mean(ckt_valid), \"novel\": np.mean(novel), \"n\": len(graphs)}\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    cache_dir = 'cache/api_ckt_ednce/'\n",
    "    folder = hash_args(args)\n",
    "    setattr(args, \"folder\", folder)\n",
    "    os.makedirs(f'ckpts/api_ckt_ednce/{folder}', exist_ok=True)\n",
    "    os.makedirs(f'cache/api_ckt_ednce/{folder}', exist_ok=True)\n",
    "    #json.dumps(args.__dict__, folder)\n",
    "    args_path = os.path.join(f'ckpts/api_ckt_ednce/{folder}', \"args.txt\")\n",
    "    with open(args_path, \"w\") as f:\n",
    "        for arg_name, arg_value in sorted(args.__dict__.items()):\n",
    "            f.write(f\"{arg_name}: {arg_value}\\n\")\n",
    "    num_graphs = 10000\n",
    "    version = get_next_version(cache_dir)-1    \n",
    "    logger.info(f\"loading version {version}\")\n",
    "    grammar, anno, g = pickle.load(open(os.path.join(cache_dir, f'{version}.pkl'),'rb'))    \n",
    "    orig = load_ckt(args, load_all=True)\n",
    "    train_data, test_data, token2rule = load_data(args, anno, grammar, orig, cache_dir, num_graphs)\n",
    "    if args.datapkl:\n",
    "        print(f'The folder being written to is: {args.datapkl}')\n",
    "    else:\n",
    "        print(f'The folder being written to is: {folder}')        \n",
    "    # prepare y\n",
    "    # TODO: remove this later\n",
    "    indices = list(range(num_graphs))\n",
    "    # random.Random(0).shuffle(indices)\n",
    "    train_indices, test_indices = indices[:int(num_graphs*0.9)], indices[int(num_graphs*0.9):]    \n",
    "    y = load_y(orig, num_graphs)    \n",
    "    y = np.array(y)\n",
    "    train_y = y[train_indices, None]\n",
    "    mean_train_y = np.mean(train_y)\n",
    "    std_train_y = np.std(train_y)    \n",
    "    test_y = y[test_indices, None]\n",
    "    train_y = (train_y-mean_train_y)/std_train_y\n",
    "    test_y = (test_y-mean_train_y)/std_train_y    \n",
    "    model = train(args, train_data, test_data)\n",
    "    breakpoint()\n",
    "    bo(args, grammar, model, token2rule, train_y, test_y)\n",
    "    graphs = interactive_sample_sequences(args, model, grammar, token2rule, max_seq_len=MAX_SEQ_LEN, unique=False, visualize=False)    \n",
    "    orig_graphs = [nx.induced_subgraph(orig, orig.comps[i]) for i in range(num_graphs)]\n",
    "    metrics = evaluate(orig_graphs, graphs)\n",
    "    print(metrics)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    from src.grammar.common import get_parser\n",
    "    parser = get_parser()\n",
    "    # data hparams\n",
    "    parser.add_argument(\"--num-samples\", type=int, default=100)\n",
    "    parser.add_argument(\"--sample-batch-size\", type=int, default=10)\n",
    "    # nn hparams\n",
    "    parser.add_argument(\"--latent-dim\", type=int, default=256)\n",
    "    parser.add_argument(\"--embed-dim\", type=int, default=256)\n",
    "    parser.add_argument(\"--encoder-layers\", type=int, default=4)\n",
    "    parser.add_argument(\"--decoder-layers\", type=int, default=4)\n",
    "    parser.add_argument(\"--encoder\", choices=[\"TOKEN_GNN\", \"GNN\", \"TOKEN\"], default=\"GNN\")\n",
    "    # training\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=256)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=500)\n",
    "    parser.add_argument(\"--cuda\", default='cpu')\n",
    "    parser.add_argument(\"--datapkl\", help=\"path to folder\")\n",
    "    parser.add_argument(\"--klcoeff\", type=float, default=0.5, help=\"coefficient to KL div term in VAE loss\")\n",
    "    # eval\n",
    "    parser.add_argument(\"--max-iter\", type=int, default=500)\n",
    "    parser.add_argument(\"--max-ei-iter\", type=int, default=150)\n",
    "    args = parser.parse_args([\n",
    "        \"--cuda\", \"cpu\",\n",
    "        \"--datapkl\", \"20c99ac6d2a2e3302b8a88f0bbdb0b99\",\n",
    "        \"--checkpoint\", \"14\",\n",
    "        \"--epochs\", \"14\",\n",
    "        \"--max-ei-iter\", \"1\",\n",
    "        \"--max-iter\", \"1\",\n",
    "        \"--BO-rounds\", \"2\",\n",
    "        \"--BO-batch-size\", \"2\",\n",
    "        \"--num-samples\", \"5\",\n",
    "        \"--sample-batch-size\", \"2\"\n",
    "    ])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "cache_dir = 'cache/api_ckt_ednce/'\n",
    "folder = hash_args(args)\n",
    "setattr(args, \"folder\", folder)\n",
    "os.makedirs(f'ckpts/api_ckt_ednce/{folder}', exist_ok=True)\n",
    "os.makedirs(f'cache/api_ckt_ednce/{folder}', exist_ok=True)\n",
    "#json.dumps(args.__dict__, folder)\n",
    "args_path = os.path.join(f'ckpts/api_ckt_ednce/{folder}', \"args.txt\")\n",
    "with open(args_path, \"w\") as f:\n",
    "    for arg_name, arg_value in sorted(args.__dict__.items()):\n",
    "        f.write(f\"{arg_name}: {arg_value}\\n\")\n",
    "num_graphs = 10000\n",
    "version = get_next_version(cache_dir)-1    \n",
    "logger.info(f\"loading version {version}\")\n",
    "grammar, anno, g = pickle.load(open(os.path.join(cache_dir, f'{version}.pkl'),'rb'))    \n",
    "orig = load_ckt(args, load_all=True)\n",
    "train_data, test_data, token2rule = load_data(args, anno, grammar, orig, cache_dir, num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.datapkl:\n",
    "    print(f'The folder being written to is: {args.datapkl}')\n",
    "else:\n",
    "    print(f'The folder being written to is: {folder}')        \n",
    "# prepare y\n",
    "# TODO: remove this later\n",
    "indices = list(range(num_graphs))\n",
    "# random.Random(0).shuffle(indices)\n",
    "train_indices, test_indices = indices[:int(num_graphs*0.9)], indices[int(num_graphs*0.9):]    \n",
    "y = load_y(orig, num_graphs)    \n",
    "y = np.array(y)\n",
    "train_y = y[train_indices, None]\n",
    "mean_train_y = np.mean(train_y)\n",
    "std_train_y = np.std(train_y)    \n",
    "test_y = y[test_indices, None]\n",
    "train_y = (train_y-mean_train_y)/std_train_y\n",
    "test_y = (test_y-mean_train_y)/std_train_y    \n",
    "model = train(args, train_data, test_data)\n",
    "bo(args, grammar, model, token2rule, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
