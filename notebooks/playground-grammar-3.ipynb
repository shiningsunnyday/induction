{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msun415/miniconda3/envs/ckt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"config\"] = \"/home/msun415/induction/src/config/mol.yaml\"\n",
    "import sys\n",
    "os.chdir('/home/msun415/induction/')\n",
    "sys.path.append('/home/msun415/induction/my_data_efficient_grammar')\n",
    "import argparse \n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdchem\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "from functools import reduce\n",
    "from rdkit.Chem import Draw\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "import networkx as nx\n",
    "from networkx.algorithms.isomorphism import GraphMatcher\n",
    "from collections import defaultdict, Counter\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import json\n",
    "from rdkit.Chem.rdmolops import FastFindRings\n",
    "from itertools import accumulate, product\n",
    "from copy import deepcopy\n",
    "from private.molecule_graph import MolGraph\n",
    "import networkx.algorithms.chordal as chordal\n",
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from src.draw.graph import draw_graph\n",
    "import networkx as nx\n",
    "import random\n",
    "from collections.abc import Iterable\n",
    "import re\n",
    "import pickle\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from src.algo.hg import llm_call, grammar_inference\n",
    "from src.grammar.hg import Grammar\n",
    "import concurrent.futures\n",
    "\n",
    "def flatten(nested_iterable):\n",
    "    if isinstance(nested_iterable, Iterable):\n",
    "        return sum([flatten(iterable) for iterable in nested_iterable], [])\n",
    "    else:\n",
    "        return [nested_iterable]\n",
    "\n",
    "# SEED = 0\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# import pygsp as gsp\n",
    "# from pygsp import graphs\n",
    "\n",
    "from src.api.get_motifs import prepare_images\n",
    "import openai\n",
    "openai.api_key = open('notebooks/api_key.txt').readline().rstrip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seeds = 5\n",
    "for dataset in ['acrylates',]:\n",
    "    for ablate in ['ablate_merge','ablate_root','ablate_tree']:\n",
    "        seedstr_lambda = lambda seed: f\"{ablate}-{seed}\"\n",
    "\n",
    "        for seed in range(1,num_seeds+1):\n",
    "            seedstr = seedstr_lambda(seed)\n",
    "            globals()[f'grammar_{seed}'] = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-{seedstr}.pkl', 'rb'))\n",
    "            globals()[f'trees_{seed}'] = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/trees-{dataset}-{seedstr}.pkl', 'rb'))\n",
    "        grammar = deepcopy(grammar_1)    \n",
    "        for i in range(2,num_seeds+1):\n",
    "            grammar.combine(locals()[f'grammar_{i}'])\n",
    "            pickle.dump(grammar, open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-{ablate}-1-{i}-all.pkl', 'wb+'))       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seeds = 10\n",
    "for dataset in ['isocyanates']:\n",
    "    seedstr_lambda = lambda seed: f\"{seed}-more-specific\"\n",
    "    for seed in range(1,num_seeds+1):\n",
    "        seedstr = seedstr_lambda(seed)\n",
    "        globals()[f'grammar_{seed}'] = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-{seedstr}.pkl', 'rb'))\n",
    "        globals()[f'trees_{seed}'] = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/trees-{dataset}-{seedstr}.pkl', 'rb'))\n",
    "    grammar = deepcopy(grammar_1)    \n",
    "    for i in range(2,num_seeds+1):\n",
    "        grammar.combine(locals()[f'grammar_{i}'])\n",
    "        pickle.dump(grammar, open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-1-{i}-all.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seeds = 5\n",
    "for dataset in ['hopv']:\n",
    "    seedstr_lambda = lambda seed: f\"text-{seed}\"\n",
    "\n",
    "    for seed in range(1,num_seeds+1):\n",
    "        seedstr = seedstr_lambda(seed)\n",
    "        globals()[f'grammar_{seed}'] = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-{seedstr}.pkl', 'rb'))\n",
    "        globals()[f'trees_{seed}'] = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/trees-{dataset}-{seedstr}.pkl', 'rb'))\n",
    "    grammar = deepcopy(grammar_1)    \n",
    "    for i in range(2,num_seeds+1):\n",
    "        grammar.combine(locals()[f'grammar_{i}'])\n",
    "        pickle.dump(grammar, open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-text-1-{i}-all.pkl', 'wb+'))       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seeds = 10\n",
    "dataset = 'acrylates'\n",
    "seedstr_lambda = lambda seed: f\"{seed}-more-specific\"\n",
    "for seed in range(1,num_seeds+1):\n",
    "    seedstr = seedstr_lambda(seed)\n",
    "    globals()[f'grammar_{seed}'] = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-{seedstr}.pkl', 'rb'))\n",
    "    globals()[f'trees_{seed}'] = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/trees-{dataset}-{seedstr}.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in os.listdir('/home/msun415/induction/data/api_mol_hg/'):\n",
    "    if p.endswith('.pkl'):\n",
    "        data = pickle.load(open(os.path.join('/home/msun415/induction/data/api_mol_hg/', p), 'rb'))\n",
    "        if hasattr(data, 'folder_lookup'):\n",
    "            for f in data.folder_lookup.values():\n",
    "                if f is not None:\n",
    "                    path = os.path.join('/home/msun415/induction/', f)\n",
    "                    if os.path.exists(path):\n",
    "                        for d in os.listdir(path):\n",
    "                            if 'tree' in d:\n",
    "                                print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(1,num_seeds+1):\n",
    "    globals()[f'smis_{seed}'] = list(globals()[f'grammar_{seed}'].mol_lookup)\n",
    "smile_set = set(smis_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rhs(r1, r2):\n",
    "    gm = GraphMatcher(r1, r2, node_match=lambda x,y: x['label']==y['label'])\n",
    "    for sub in gm.isomorphisms_iter():\n",
    "        map_ext_nodes = set([sub[n] for n in r1 if n[0] == 'e'])\n",
    "        if map_ext_nodes == set([n for n in r2 if n[0] == 'e']):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def compare(set_1, set_2, iou=False):\n",
    "    if not iou and len(set_1) != len(set_2):\n",
    "        return False\n",
    "    if iou:\n",
    "        intersect = 0\n",
    "    for i in range(len(set_1)-1,-1,-1):\n",
    "        # try to find it in set_2\n",
    "        r1 = set_1[i]\n",
    "        found = -1\n",
    "        for j in range(len(set_2)-1,-1,-1):\n",
    "            r2 = set_2[j]\n",
    "            if len(r1) != len(r2):\n",
    "                continue\n",
    "            if compare_rhs(r1, r2):\n",
    "                found = j\n",
    "                break\n",
    "            if found != -1:\n",
    "                break\n",
    "        if not iou:\n",
    "            if found == -1:\n",
    "                return False\n",
    "            set_2.pop(found)\n",
    "        if iou and found != -1:\n",
    "            intersect += 1            \n",
    "    if iou:\n",
    "        return intersect/(len(set_1)+len(set_2)-intersect)\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "def compare_rulesets():\n",
    "    rule_multisets = defaultdict(list)\n",
    "    for smi in list(smile_set):\n",
    "        for grammar, trees, smis in [(globals()[f'grammar_{seed}'], globals()[f'trees_{seed}'], globals()[f'smis_{seed}']) for seed in range(1,num_seeds+1)]:    \n",
    "            idx = smis.index(smi)    \n",
    "            rule_gs = []\n",
    "            for i in range(len(trees[idx])):\n",
    "                data = trees[idx].nodes[i]\n",
    "                symbol = data['symbol']\n",
    "                rule_str = data['rule']\n",
    "                rule_idx = grammar.rule_idx_lookup[smi][symbol][rule_str]\n",
    "                rule = grammar.hrg.rules[rule_idx]\n",
    "                rhs_g = rule.rhs.visualize(path='', return_g=True)\n",
    "                rule_gs.append(rhs_g)\n",
    "            rule_multisets[smi].append(rule_gs)\n",
    "    # compare two rule sets\n",
    "    res = {}\n",
    "    for smiles in rule_multisets:\n",
    "        multisets = rule_multisets[smiles]\n",
    "        count = 0\n",
    "        for i in range(len(multisets)):\n",
    "            for j in range(i+1, len(multisets)):\n",
    "                count += compare(multisets[i], multisets[j], iou=True)\n",
    "        res[smiles] = count/(len(multisets)*(len(multisets)-1)/2)\n",
    "\n",
    "\n",
    "def bradley_terry(pairwise_results, num_debaters, max_iterations=100, tolerance=1e-6):\n",
    "    # Step 1: Initialize skill levels for each debater (theta values)\n",
    "    theta = [1.0 for _ in range(num_debaters)]  # Initial guess, could be any positive number\n",
    "    for iteration in range(max_iterations):\n",
    "        theta_old = theta[:]  # Keep a copy of the old theta values for convergence check\n",
    "        # Step 2: For each debater i, update their theta based on all pairwise matches\n",
    "        for i in range(num_debaters):\n",
    "            numerator = 0.0\n",
    "            denominator = 0.0\n",
    "            # Step 3: Iterate over all match results involving debater i\n",
    "            for (i_prime, j, S_i_prime) in pairwise_results:\n",
    "                if i_prime == i:  # i played against j\n",
    "                    numerator += S_i_prime  # Sum of debater i's scores\n",
    "                    denominator += S_i_prime + (1 - S_i_prime) * (theta[j] / theta[i])  # Bradley-Terry denominator\n",
    "                elif j == i:  # i played against i_prime\n",
    "                    numerator += (1 - S_i_prime)  # Sum of debater j's scores\n",
    "                    denominator += (1 - S_i_prime) + S_i_prime * (theta[i_prime] / theta[i])\n",
    "            # Step 4: Update theta for debater i using the Bradley-Terry scaling formula\n",
    "            theta[i] = numerator / denominator\n",
    "        # Step 5: Check for convergence\n",
    "        if max(abs(theta[i] - theta_old[i]) for i in range(num_debaters)) < tolerance:\n",
    "            break\n",
    "    # Step 6: Rank debaters by theta values (higher is better)\n",
    "    ranked_debaters = sorted(range(num_debaters), key=lambda x: theta[x], reverse=True)\n",
    "    return ranked_debaters, theta  # The winner is the debater with the highest theta value\n",
    "\n",
    "\n",
    "def llm_debate(smiles, cot_1, cot_2, prompt_path):\n",
    "    base_prompt = ''.join(open(prompt_path).readlines())\n",
    "    base_prompt = base_prompt.replace('<smiles>', smiles)\n",
    "    prompt = f\"Analysis A:\\n\\n{cot_1}\\n\\nAnalysis B:\\n\\n{cot_2}\"\n",
    "    logprobs = llm_call([], None, prompt=base_prompt+'\\n'+prompt, return_logprobs=True)\n",
    "    log_probs = [None, None]\n",
    "    for logprob in logprobs:\n",
    "        if logprob['token'] == 'A':\n",
    "            log_probs[0] = logprob['logprob']\n",
    "        elif logprob['token'] == 'B':\n",
    "            log_probs[1] = logprob['logprob']\n",
    "    if None in log_probs:\n",
    "        return None\n",
    "    return np.exp(log_probs)/sum(np.exp(log_probs))\n",
    "\n",
    "\n",
    "def read_tree_cot(folder):\n",
    "    path_1 = os.path.join(folder, 'motifs_cot.txt')\n",
    "    path_2 = os.path.join(folder, 'root_cot.txt')\n",
    "    lines_1 = open(path_1).readlines()\n",
    "    lines_2 = open(path_2).readlines()\n",
    "    clique_cot = ''.join(lines_1)\n",
    "    root_cot = ''.join(lines_2)\n",
    "    paths = glob.glob(os.path.join(folder, 'tree/*.txt'))\n",
    "    paths.sort(key=lambda x: os.path.getmtime(x))\n",
    "    tree_cots = []\n",
    "    for i, path in enumerate(paths):    \n",
    "        stem = Path(path).stem\n",
    "        nodes = stem.split(':')[0].split('-')\n",
    "        a, b = map(int, nodes)\n",
    "        step = f\"Step {i+1}: Join Motif {a} (positive) with Motif {b} (negative).\\n\"\n",
    "        step_cot = ''.join(open(path).readlines())\n",
    "        cot = step + f\"Reasoning: {step_cot}\\n\"\n",
    "        tree_cots.append(cot)\n",
    "    tree_cot = clique_cot + '\\n\\n' + root_cot + '\\n\\n' + '\\n'.join(tree_cots)\n",
    "    return tree_cot\n",
    "\n",
    "\n",
    "def round_robin_tournament(smis):\n",
    "    # rank which grammar did the best job at decomposing a mol\n",
    "    winner = {}\n",
    "    rankings = {}\n",
    "    for smi in smis:\n",
    "        pairwise_res = []\n",
    "        tree_cots = [None for _ in range(num_seeds)]\n",
    "        for i in range(num_seeds):\n",
    "            folder = globals()[f'grammar_{i+1}'].folder_lookup[smi]\n",
    "            tree_cots[i] = read_tree_cot(folder)\n",
    "        for i in range(num_seeds):\n",
    "            for j in range(i+1, num_seeds):\n",
    "                res = llm_debate(smi, tree_cots[i], tree_cots[j], f'/home/msun415/induction/data/{dataset}/api_mol_hg_7.txt')\n",
    "                print(res)\n",
    "                pairwise_res.append((i, j, res[0]))\n",
    "        ranking, theta = bradley_terry(pairwise_res, num_seeds)\n",
    "        winner[smi] = ranking[0]\n",
    "        rankings[smi] = ranking \n",
    "    return winner, rankings  \n",
    "\n",
    "\n",
    "def swiss_tournament(smis, num_rounds=4, cache=None):\n",
    "    def _tournament(smi):\n",
    "        tree_cots = [None for _ in range(num_seeds)]        \n",
    "        for i in range(num_seeds):\n",
    "            folder = globals()[f'grammar_{i+1}'].folder_lookup[smi]\n",
    "            tree_cots[i] = read_tree_cot(folder)\n",
    "        pairwise_results = []\n",
    "        debater_scores = {i: 0 for i in range(num_seeds)}\n",
    "        for round_num in range(num_rounds):\n",
    "            sorted_debaters = sorted(debater_scores.keys(), key=lambda x: debater_scores[x], reverse=True)\n",
    "            if num_seeds % 2:\n",
    "                pairings = range(round_num%2, num_seeds-(round_num+1)%2, 2)\n",
    "            else:\n",
    "                pairings = range(0,num_seeds-1,2)\n",
    "            match_pairs = [(sorted_debaters[i], sorted_debaters[i + 1]) for i in pairings]\n",
    "            for i, j in match_pairs:                \n",
    "                res = llm_debate(smi, tree_cots[i], tree_cots[j], f'/home/msun415/induction/data/{dataset}/api_mol_hg_7.txt')  \n",
    "                S_i = res[0]              \n",
    "                S_j = 1 - S_i\n",
    "                if S_i == 1 or S_i == 0:\n",
    "                    print(\"bad\")\n",
    "                pairwise_results.append((i, j, S_i))\n",
    "                debater_scores[i] += S_i\n",
    "                debater_scores[j] += S_j\n",
    "        return smi, pairwise_results\n",
    "    if cache and os.path.exists(cache):\n",
    "        winner, rankings = pickle.load(open(cache, 'rb'))\n",
    "        smis = [smi for smi in smis if smi not in winner]\n",
    "    else:\n",
    "        winner = {}\n",
    "        rankings = {}        \n",
    "    import threading \n",
    "    lock = threading.Lock()\n",
    "    with concurrent.futures.ThreadPoolExecutor(50) as executor:\n",
    "        # Submit tasks to the thread pool\n",
    "        futures = [\n",
    "            executor.submit(_tournament, smi)\n",
    "            for smi in tqdm(smis, desc=\"submitting generation tasks\")\n",
    "        ]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            smi, pairwise_results = future.result()\n",
    "            ranked_debaters, _ = bradley_terry(pairwise_results, num_seeds)\n",
    "            with lock:\n",
    "                winner[smi] = ranked_debaters[0]\n",
    "                rankings[smi] = ranked_debaters\n",
    "                pickle.dump((winner, rankings), open(cache, 'wb+'))\n",
    "                print(f\"{len(winner)}/{len(smis)} done\")\n",
    "    return winner, rankings \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "submitting generation tasks: 100%|██████████| 316/316 [00:00<00:00, 1757.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/316 done\n",
      "2/316 done\n",
      "3/316 done\n",
      "4/316 done\n",
      "5/316 done\n",
      "6/316 done\n",
      "7/316 done\n",
      "8/316 done\n",
      "9/316 done\n",
      "10/316 done\n",
      "11/316 done\n",
      "12/316 done\n",
      "13/316 done\n",
      "14/316 done\n",
      "15/316 done\n",
      "16/316 done\n",
      "17/316 done\n",
      "18/316 done\n",
      "19/316 done\n",
      "20/316 done\n",
      "21/316 done\n",
      "22/316 done\n",
      "23/316 done\n",
      "24/316 done\n",
      "25/316 done\n",
      "26/316 done\n",
      "27/316 done\n",
      "28/316 done\n",
      "29/316 done\n",
      "30/316 done\n",
      "31/316 done\n",
      "32/316 done\n",
      "33/316 done\n",
      "34/316 done\n",
      "35/316 done\n",
      "36/316 done\n",
      "37/316 done\n",
      "38/316 done\n",
      "39/316 done\n",
      "40/316 done\n",
      "41/316 done\n",
      "42/316 done\n",
      "43/316 done\n",
      "44/316 done\n",
      "45/316 done\n",
      "46/316 done\n",
      "47/316 done\n",
      "48/316 done\n",
      "49/316 done\n",
      "50/316 done\n",
      "51/316 done\n",
      "52/316 done\n",
      "53/316 done\n",
      "54/316 done\n",
      "55/316 done\n",
      "56/316 done\n",
      "57/316 done\n",
      "58/316 done\n",
      "59/316 done\n",
      "60/316 done\n",
      "61/316 done\n",
      "62/316 done\n",
      "63/316 done\n",
      "64/316 done\n",
      "65/316 done\n",
      "66/316 done\n",
      "67/316 done\n",
      "68/316 done\n",
      "69/316 done\n",
      "70/316 done\n",
      "71/316 done\n",
      "72/316 done\n",
      "73/316 done\n",
      "74/316 done\n",
      "75/316 done\n",
      "76/316 done\n",
      "77/316 done\n",
      "78/316 done\n",
      "79/316 done\n",
      "80/316 done\n",
      "81/316 done\n",
      "82/316 done\n",
      "83/316 done\n",
      "84/316 done\n",
      "85/316 done\n",
      "86/316 done\n",
      "87/316 done\n",
      "88/316 done\n",
      "89/316 done\n",
      "90/316 done\n",
      "91/316 done\n",
      "92/316 done\n",
      "93/316 done\n",
      "94/316 done\n",
      "95/316 done\n",
      "96/316 done\n",
      "97/316 done\n",
      "98/316 done\n",
      "99/316 done\n",
      "100/316 done\n",
      "101/316 done\n",
      "102/316 done\n",
      "103/316 done\n",
      "104/316 done\n",
      "105/316 done\n",
      "106/316 done\n",
      "107/316 done\n",
      "108/316 done\n",
      "109/316 done\n",
      "110/316 done\n",
      "111/316 done\n",
      "112/316 done\n",
      "113/316 done\n",
      "114/316 done\n",
      "115/316 done\n",
      "116/316 done\n",
      "117/316 done\n",
      "118/316 done\n",
      "119/316 done\n",
      "120/316 done\n",
      "121/316 done\n",
      "122/316 done\n",
      "123/316 done\n",
      "124/316 done\n",
      "125/316 done\n",
      "126/316 done\n",
      "127/316 done\n",
      "128/316 done\n",
      "129/316 done\n",
      "130/316 done\n",
      "131/316 done\n",
      "132/316 done\n",
      "133/316 done\n",
      "134/316 done\n",
      "135/316 done\n",
      "136/316 done\n",
      "137/316 done\n",
      "138/316 done\n",
      "139/316 done\n",
      "140/316 done\n",
      "141/316 done\n",
      "142/316 done\n",
      "143/316 done\n",
      "144/316 done\n",
      "145/316 done\n",
      "146/316 done\n",
      "147/316 done\n",
      "148/316 done\n",
      "149/316 done\n",
      "150/316 done\n",
      "151/316 done\n",
      "152/316 done\n",
      "153/316 done\n",
      "154/316 done\n",
      "155/316 done\n",
      "156/316 done\n",
      "157/316 done\n",
      "158/316 done\n",
      "159/316 done\n",
      "160/316 done\n",
      "161/316 done\n",
      "162/316 done\n",
      "163/316 done\n",
      "164/316 done\n",
      "165/316 done\n",
      "166/316 done\n",
      "167/316 done\n",
      "168/316 done\n",
      "169/316 done\n",
      "170/316 done\n",
      "171/316 done\n",
      "172/316 done\n",
      "173/316 done\n",
      "174/316 done\n",
      "175/316 done\n",
      "176/316 done\n",
      "177/316 done\n",
      "178/316 done\n",
      "179/316 done\n",
      "180/316 done\n",
      "181/316 done\n",
      "182/316 done\n",
      "183/316 done\n",
      "184/316 done\n",
      "185/316 done\n",
      "186/316 done\n",
      "187/316 done\n",
      "188/316 done\n",
      "189/316 done\n",
      "190/316 done\n",
      "191/316 done\n",
      "192/316 done\n",
      "193/316 done\n",
      "194/316 done\n",
      "195/316 done\n",
      "196/316 done\n",
      "197/316 done\n",
      "198/316 done\n",
      "199/316 done\n",
      "200/316 done\n",
      "201/316 done\n",
      "202/316 done\n",
      "203/316 done\n",
      "204/316 done\n",
      "205/316 done\n",
      "206/316 done\n",
      "207/316 done\n",
      "208/316 done\n",
      "209/316 done\n",
      "210/316 done\n",
      "211/316 done\n",
      "212/316 done\n",
      "213/316 done\n",
      "214/316 done\n",
      "215/316 done\n",
      "216/316 done\n",
      "217/316 done\n",
      "218/316 done\n",
      "219/316 done\n",
      "220/316 done\n",
      "221/316 done\n",
      "222/316 done\n",
      "223/316 done\n",
      "224/316 done\n",
      "225/316 done\n",
      "226/316 done\n",
      "227/316 done\n",
      "228/316 done\n",
      "229/316 done\n",
      "230/316 done\n",
      "231/316 done\n",
      "232/316 done\n",
      "233/316 done\n",
      "234/316 done\n",
      "235/316 done\n",
      "236/316 done\n",
      "237/316 done\n",
      "238/316 done\n",
      "239/316 done\n",
      "240/316 done\n",
      "241/316 done\n",
      "242/316 done\n",
      "243/316 done\n",
      "244/316 done\n",
      "245/316 done\n",
      "246/316 done\n",
      "247/316 done\n",
      "248/316 done\n",
      "249/316 done\n",
      "250/316 done\n",
      "251/316 done\n",
      "252/316 done\n",
      "253/316 done\n",
      "254/316 done\n",
      "255/316 done\n",
      "256/316 done\n",
      "257/316 done\n",
      "258/316 done\n",
      "259/316 done\n",
      "260/316 done\n",
      "261/316 done\n",
      "262/316 done\n",
      "263/316 done\n",
      "264/316 done\n",
      "265/316 done\n",
      "266/316 done\n",
      "267/316 done\n",
      "268/316 done\n",
      "269/316 done\n",
      "270/316 done\n",
      "271/316 done\n",
      "272/316 done\n",
      "273/316 done\n",
      "274/316 done\n",
      "275/316 done\n",
      "276/316 done\n",
      "277/316 done\n",
      "278/316 done\n",
      "279/316 done\n",
      "280/316 done\n",
      "281/316 done\n",
      "282/316 done\n",
      "283/316 done\n",
      "284/316 done\n",
      "285/316 done\n",
      "286/316 done\n",
      "287/316 done\n",
      "288/316 done\n",
      "289/316 done\n",
      "290/316 done\n",
      "291/316 done\n",
      "292/316 done\n",
      "293/316 done\n",
      "294/316 done\n",
      "295/316 done\n",
      "296/316 done\n",
      "297/316 done\n",
      "298/316 done\n",
      "299/316 done\n",
      "300/316 done\n",
      "301/316 done\n",
      "302/316 done\n",
      "303/316 done\n",
      "304/316 done\n",
      "305/316 done\n",
      "306/316 done\n",
      "307/316 done\n",
      "308/316 done\n",
      "309/316 done\n",
      "310/316 done\n",
      "311/316 done\n",
      "312/316 done\n",
      "313/316 done\n",
      "314/316 done\n",
      "315/316 done\n",
      "316/316 done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "smis = list(grammar_1.mol_lookup)\n",
    "# winner, rankings = swiss_tournament(smis, num_rounds=4)\n",
    "cache_path = f\"data/api_mol_hg/{dataset}-swiss-text.pkl\"\n",
    "winner, rankings = swiss_tournament(smis, num_rounds=4, cache=cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_count = defaultdict(int)\n",
    "for smi in winner:\n",
    "    winner_count[winner[smi]] += 1\n",
    "winner_count\n",
    "\n",
    "# pickle.dump((winner, rankings), open('/home/msun415/winner_and_rankings_tmp.pkl', 'wb+'))\n",
    "# winner, rankings = pickle.load(open('/home/msun415/winner_and_rankings_tmp.pkl', 'rb'))\n",
    "\n",
    "# bad_smiles = 'Cc1c(-c2c3nc4c(nc3c(-c3c(C)cc(-c5ccc6-c7ccccc7C(C)(C)c6c5)s3)s2)-c2cccc3cccc-4c23)scc1'\n",
    "# bad_smiles in grammar_1.rule_idx_lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check = []\n",
    "orig_smiles = open('data/api_mol_hg/hopv_smiles.txt').readlines()\n",
    "# print(len(orig_smiles))\n",
    "orig_smi_set = set()\n",
    "for orig_smi, smi in zip(orig_smiles, smis):\n",
    "    mol = Chem.MolFromSmiles(orig_smi)\n",
    "    Chem.Kekulize(mol)\n",
    "    orig_smi_set.add(Chem.MolToSmiles(mol))\n",
    "print(orig_smi_set.difference(set(smis)))\n",
    "print(set(smis).difference(orig_smi_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1356536/2545462448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_seeds\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"k\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mG_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrees_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msmi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"combining rule sets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(1,num_seeds+1), \"k\"):\n",
    "    G_pool = None\n",
    "    trees_pool = defaultdict(list)\n",
    "    for smi in tqdm(smis, \"combining rule sets\"):\n",
    "        for i in range(k):\n",
    "            r = rankings[smi][i]\n",
    "            winner_g = locals()[f'grammar_{r+1}']\n",
    "            winner_trees = locals()[f'trees_{r+1}']\n",
    "            tree_index = list(winner_g.mol_lookup).index(smi)\n",
    "            winner_tree = winner_trees[tree_index]\n",
    "            trees_pool[smi] += [winner_tree]\n",
    "            rule = winner_g.mol_lookup[smi]\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            g = Grammar(mol, rule)\n",
    "            Grammar._init_mol_lookup(g)\n",
    "            if G_pool is None:\n",
    "                G_pool = g\n",
    "            else:\n",
    "                G_pool.combine(g)\n",
    "    grammar_inference(G_pool, trees_pool)\n",
    "    pickle.dump(G_pool, open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-swiss-pool-text-{k}.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moses\n",
    "\n",
    "dataset = 'polymers_117'\n",
    "seed = 1\n",
    "gen_smiles = open(f'/home/msun415/induction/data/api_mol_hg/smiles-{dataset}-{seed}-more-specific.txt').readlines()\n",
    "test_smiles = open(f'/home/msun415/induction/data/api_mol_hg/polymers_117_test_smiles.txt').readlines()\n",
    "train_smiles = open(f'/home/msun415/induction/data/api_mol_hg/polymers_117_train_smiles.txt').readlines()\n",
    "# moses.get_all_metrics(gen_smiles[:10000], test=test_smiles, n_jobs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem.QED import qed\n",
    "from moses.metrics.SA_Score import sascorer\n",
    "def logP(mol):\n",
    "    return Chem.Crippen.MolLogP(mol)\n",
    "def SA(mol):\n",
    "    return sascorer.calculateScore(mol)\n",
    "def QED(mol):\n",
    "    return qed(mol)\n",
    "def weight(mol):\n",
    "    return Descriptors.MolWt(mol)\n",
    "def features(mol):\n",
    "    return [logP(mol), SA(mol), QED(mol), weight(mol)]\n",
    "\n",
    "import multiprocessing as mp\n",
    "def fitting(gen_smiles, train_smiles, num_clusters=10000):\n",
    "    with mp.Pool(100) as p:\n",
    "        gen_feats = p.map(features, [Chem.MolFromSmiles(smi) for smi in tqdm(gen_smiles)])\n",
    "    with mp.Pool(100) as p:\n",
    "        train_feats = p.map(features, [Chem.MolFromSmiles(smi) for smi in tqdm(train_smiles)])\n",
    "    from sklearn.cluster import KMeans\n",
    "    print(\"fit start\")\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, verbose=1, n_init=1).fit(train_feats)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    print(\"fit finish\")\n",
    "    cluster_assignments = kmeans.predict(gen_feats)\n",
    "    clusters = [set() for _ in range(num_clusters)]\n",
    "    for i in range(len(gen_feats)):\n",
    "        clusters[cluster_assignments[i]].add(i)\n",
    "    train_cluster_assignments = kmeans.predict(train_feats)\n",
    "    print(\"predict finish\")\n",
    "    print(\"train\", train_cluster_assignments)\n",
    "    print(\"gen\", cluster_assignments)\n",
    "    final_samples = []\n",
    "    counts = defaultdict(int)\n",
    "    for i in range(num_clusters):        \n",
    "        order = sorted(range(num_clusters), key=lambda j:np.linalg.norm(centroids[j]-centroids[i]))\n",
    "        assert order[0] == i\n",
    "        for ci, c in enumerate(order):\n",
    "            if len(clusters[c]):\n",
    "                sample = random.choice(list(clusters[c]))\n",
    "                clusters[c].remove(sample)\n",
    "                counts[ci] += 1\n",
    "                break\n",
    "        final_samples.append(sample)\n",
    "    # print(f\"{len(final_samples)} equal mass samples\")\n",
    "    print(f\"{counts[0]} equal mass samples\")\n",
    "    return np.array(final_samples)\n",
    "\n",
    "fitted = fitting(gen_smiles, train_smiles, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moses.get_all_metrics(gen_smiles[:10000], test=test_smiles, n_jobs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_smiles_list = []\n",
    "dataset = 'acrylates'\n",
    "G = pickle.load(open(f'data/api_mol_hg/grammar-{dataset}-2-more-specific.pkl','rb'))\n",
    "# folders = ['.2498708','.7247128','.9170787','.164053','.4041026','.0848994','.337852','.0067797','.9718816']\n",
    "# # folders = ['.7506363','.8882577','.3260689','.0348148','.9410613','.2453573','.2190194','.1925354','.192024','.2320652']\n",
    "# inv_lookup = dict(zip(G.folder_lookup.values(), G.folder_lookup.keys()))\n",
    "# for f in folders:\n",
    "#     key = None\n",
    "#     for k in inv_lookup: \n",
    "#         if f in k:\n",
    "#             if key:\n",
    "#                 print(\"BAD\")\n",
    "#             key = k\n",
    "#     print(f, inv_lookup[key])\n",
    "#     debug_smiles_list.append(inv_lookup[key])\n",
    "\n",
    "\n",
    "# # for l in open('data/api_mol_hg/hopv_smiles.txt').readlines():\n",
    "# #     mol = Chem.MolFromSmiles(l)\n",
    "# #     Chem.Kekulize(mol)\n",
    "# #     smi = Chem.MolToSmiles(mol)\n",
    "# #     assert smi in debug_smiles_list\n",
    "# # '.2320652' in G.folder_lookup['COP(=O)(OC)OC=C(Cl)Cl']\n",
    "# choice = np.random.choice(list(G.folder_lookup))\n",
    "# assert choice not in debug_smiles_list\n",
    "# debug_smiles_list += [choice]\n",
    "# print(debug_smiles_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('zip -r acrylates-v2.zip ' + ' '.join(G.folder_lookup.values()) + ' data/api_mol_hg-acrylates-2-more-specific.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_chain_extender(smis):\n",
    "    # Define SMARTS patterns for various chain extenders\n",
    "    members = 0\n",
    "    for smi in smis:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        members += mol.HasSubstructMatch(Chem.MolFromSmiles(\"CO\")) or mol.HasSubstructMatch(Chem.MolFromSmiles(\"CN\"))\n",
    "    return members/len(smis)\n",
    "\n",
    "\n",
    "start = [1,1,1,2,2]\n",
    "num_seeds = [10,10,10,5,5]\n",
    "start_dataset = 0\n",
    "num_datasets = 5\n",
    "fig, axes = plt.subplots(1,num_datasets,figsize=(10,5))\n",
    "for j in range(start_dataset,num_datasets):\n",
    "    dataset = ['isocyanates','acrylates','chain_extenders','hopv','ptc'][j]\n",
    "    titles = ['Isocyanates','Acrylates','Chain Extenders','HOPV','PTC']\n",
    "    ax = axes[j]\n",
    "    for i in range(2):\n",
    "        memb, rs, novelty, unique, diversity = [], [], [], [], []    \n",
    "        setting = \"Top k\" if i == 0 else \"1-k\"\n",
    "        for k in range(start[j],num_seeds[j]+1):\n",
    "            if k == 1 and i == 1:\n",
    "                seedstr = \"1-more-specific\"\n",
    "            else:\n",
    "                seedstr = f\"swiss-pool-{k}\" if i == 0 else f\"1-{k}-all\"\n",
    "            res = json.load(open(f'/home/msun415/induction/data/api_mol_hg/metrics-for-smiles-{dataset}-{seedstr}.json'))\n",
    "            if dataset == 'chain_extenders':\n",
    "                smis = open(f'/home/msun415/induction/data/api_mol_hg/smiles-{dataset}-{seedstr}.txt').readlines()\n",
    "                res['membership'] = check_chain_extender(smis)            \n",
    "                \n",
    "            memb.append(res['membership'])\n",
    "            rs.append(res['RS'])\n",
    "            novelty.append(res['novelty'])\n",
    "            unique.append(res['unique'])\n",
    "            diversity.append(res['diversity'])\n",
    "        ax.set_xticks(range(start[j],num_seeds[j]+1))\n",
    "        ax.plot(range(start[j],num_seeds[j]+1), memb, label=f'{setting} memb.', linestyle='--' if i==1 else '-', c='blue')\n",
    "        ax.plot(range(start[j],num_seeds[j]+1), rs, label=f'{setting} RS', linestyle='--' if i==1 else '-', c='red')\n",
    "        # ax.plot(range(1,num_seeds+1), novelty, label=f'{setting} novelty')\n",
    "        # ax.plot(range(1,num_seeds+1), unique, label=f'{setting} unique')\n",
    "        ax.plot(range(start[j],num_seeds[j]+1), diversity, label=f'{setting} diversity', linestyle='--' if i==1 else '-', c='green')\n",
    "        if j == 2:\n",
    "            ax.legend(loc='best')\n",
    "        ax.autoscale()\n",
    "        ax.set_title(titles[j])\n",
    "        ax.set_xlabel(\"k\")\n",
    "fig.savefig('ablation.pdf', bbox_layout='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_chain_extender(smis):\n",
    "    # Define SMARTS patterns for various chain extenders\n",
    "    members = 0\n",
    "    for smi in smis:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        mol.SubstructMatches(Chem.MolFromSmiles(\"CO\"))\n",
    "        members += match\n",
    "    return members/len(smis)\n",
    "\n",
    "\n",
    "def check_hopv(smis):\n",
    "    thio = Chem.MolFromSmiles('[cH:1]1[cH:2][cH:3][cH:4][s:5]1') # thiophene\n",
    "    members = 0\n",
    "    for smi in smis:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            continue\n",
    "        members += bool(mol.GetSubstructMatches(thio))\n",
    "    return members/len(smis)\n",
    "\n",
    "\n",
    "def check_acrylate(smis):\n",
    "    # Define the SMARTS pattern for the Isocyanate group (-N=C=O)\n",
    "    acrylate_smarts = 'C=CC(=O)O'\n",
    "    acrylate_pattern = Chem.MolFromSmarts(acrylate_smarts)\n",
    "    members = 0\n",
    "    for smi in smis:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        members += mol.HasSubstructMatch(acrylate_pattern)    \n",
    "    return members/len(smis)\n",
    "\n",
    "\n",
    "\n",
    "smis = open('/home/msun415/test.txt').readlines()\n",
    "check_acrylate(smis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'hopv'\n",
    "g1 = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-swiss-pool-5.pkl', 'rb'))\n",
    "g2 = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-1-5-all.pkl', 'rb'))\n",
    "print(len(g1.hrg.rules), len(g2.hrg.rules))\n",
    "rules1 = g1.hrg.rules\n",
    "rules2 = g2.hrg.rules\n",
    "\n",
    "rule_multiset1 = [rule.rhs.visualize(path='', return_g=True) for rule in rules1]\n",
    "rule_multiset2 = [rule.rhs.visualize(path='', return_g=True) for rule in rules2]\n",
    "print(compare(deepcopy(rule_multiset1), deepcopy(rule_multiset2)))\n",
    "\n",
    "\n",
    "sum(g1.hrg.counts), sum(g2.hrg.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='ptc'\n",
    "g1 = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-swiss-pool-5.pkl', 'rb'))\n",
    "g2 = pickle.load(open(f'/home/msun415/induction/data/api_mol_hg/grammar-{dataset}-1-5-all.pkl', 'rb'))\n",
    "print(len(g1.hrg.rules), len(g2.hrg.rules))\n",
    "rules1 = g1.hrg.rules\n",
    "rules2 = g2.hrg.rules\n",
    "\n",
    "rule_multiset1 = [rule.rhs.visualize(path='', return_g=True) for rule in rules1]\n",
    "rule_multiset2 = [rule.rhs.visualize(path='', return_g=True) for rule in rules2]\n",
    "print(compare(deepcopy(rule_multiset1), deepcopy(rule_multiset2)))\n",
    "\n",
    "sum(g1.hrg.counts), sum(g2.hrg.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in glob.glob('/data/api_mol_hg/grammar*.pkl'):\n",
    "    g = pickle.load(open(f, 'rb'))\n",
    "    if isinstance(g, tuple):\n",
    "        g = g[0]\n",
    "    for k, v in g.folder_lookup.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        if '8830726' in v:\n",
    "            print(k, v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
