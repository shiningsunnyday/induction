{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"config\"] = \"/home/msun415/induction/src/config/ckt.yaml\"\n",
    "import sys\n",
    "os.chdir('/home/msun415/induction/')\n",
    "from src.examples import *\n",
    "from src.draw.graph import draw_graph\n",
    "from argparse import ArgumentParser\n",
    "import pickle\n",
    "import heapq\n",
    "from src.grammar.ednce import *\n",
    "from src.draw.graph import *\n",
    "from src.api.get_motifs import *\n",
    "from src.algo.utils import *\n",
    "from src.algo.common import *\n",
    "from src.grammar.common import *\n",
    "from src.grammar.utils import *\n",
    "from src.algo.ednce import *\n",
    "from src.model import graph_regression, transformer_regression\n",
    "from argparse import ArgumentParser\n",
    "from networkx.algorithms.isomorphism import DiGraphMatcher\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    if DATASET == \"cora\":\n",
    "        g = load_cora()\n",
    "    elif DATASET == \"test\":\n",
    "        g = create_test_graph(1)\n",
    "    elif DATASET == \"debug\":\n",
    "        g = debug()\n",
    "    elif DATASET == \"house\":\n",
    "        g = create_house_graph()\n",
    "    elif DATASET == \"ckt\":\n",
    "        g = load_ckt(args)\n",
    "    elif DATASET == \"enas\":\n",
    "        g = load_enas(args)\n",
    "    elif DATASET == \"bn\":\n",
    "        g = load_bn(args)\n",
    "    elif DATASET == \"ast\":\n",
    "        g = load_ast(args)\n",
    "    elif DATASET == \"mol\":\n",
    "        g = load_mols(args)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return g\n",
    "    \n",
    "def get_args():\n",
    "    parser = ArgumentParser()\n",
    "    # global args\n",
    "    parser.add_argument(\"--visualize\", dest=\"global_visualize\", action='store_true')\n",
    "    parser.add_argument(\"--cache\", dest=\"global_cache\", action='store_true')    \n",
    "    parser.add_argument(\"--num_threads\", dest=\"global_num_threads\", type=int)\n",
    "    parser.add_argument(\"--num_procs\", dest=\"global_num_procs\", type=int)    \n",
    "    # hparams\n",
    "    parser.add_argument(\"--scheme\", choices=['one','zero'], help='whether to index from 0 or 1', default='zero')    \n",
    "    # ablations\n",
    "    parser.add_argument(\"--ablate_tree\", action='store_true') \n",
    "    parser.add_argument(\"--ablate_merge\", action='store_true') \n",
    "    parser.add_argument(\"--ablate_root\", action='store_true') \n",
    "    # task params\n",
    "    parser.add_argument(\"--task\", nargs='+', choices=[\"learn\",\"generate\",\"prediction\"])\n",
    "    parser.add_argument(\"--seed\")\n",
    "    parser.add_argument(\"--grammar_ckpt\")\n",
    "    # mol dataset args\n",
    "    parser.add_argument(\n",
    "        \"--mol-dataset\",\n",
    "        choices=[\"ptc\",\"hopv\",\"polymers_117\", \"isocyanates\", \"chain_extenders\", \"acrylates\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-data-samples\", type=int\n",
    "    )\n",
    "    parser.add_argument(\"--ambiguous-file\", help='if given and exists, load data from this file to learn grammar; if given and not exist, save ambiguous data to this file after learn grammar')\n",
    "    parser.add_argument(\"--num_samples\", default=10000, type=int, help='how much to generate')\n",
    "    return parser.parse_args([\n",
    "        '--task', 'learn',\n",
    "        '--ambiguous-file', 'cache/api_ckt_ednce/ambig_1.json'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "g = load_data(args)\n",
    "orig = deepcopy(g)\n",
    "cache_iter, cache_path = setup()\n",
    "print(cache_path)\n",
    "g, grammar, anno, iter = init_grammar(g, cache_iter, cache_path, EDNCEGrammar)\n",
    "grammar, model, anno, g = terminate(g, grammar, anno, iter)\n",
    "# for j, m in enumerate(model):\n",
    "#     pre = get_prefix(m.id)\n",
    "#     # draw_tree(m, os.path.join(IMG_DIR, f\"model_{iter}_{pre}.png\"))\n",
    "#     model[j] = EDNCEModel(dfs(anno, m.id))\n",
    "for j, m in enumerate(model):\n",
    "    pre = get_prefix(m.id)\n",
    "    # draw_tree(m, os.path.join(IMG_DIR, f\"model_{iter}_{pre}.png\"))\n",
    "    model[j] = EDNCEModel(dfs(anno, m.id))\n",
    "graphs = [copy_graph(orig, orig.comps[get_prefix(m.seq[0])]) for m in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_single(stack, grammar, graph, init_hash, mem, lock):\n",
    "    while True:\n",
    "        with lock:\n",
    "            if len(stack) == 0:            \n",
    "                if init_hash in mem and mem[init_hash] != 0:\n",
    "                    print(\"process done\")\n",
    "                    break\n",
    "                else:\n",
    "                    time.sleep(0.1)\n",
    "                    continue\n",
    "            else:\n",
    "                print(len(stack))\n",
    "                cur, val = stack.pop(-1)\n",
    "            if val in mem:\n",
    "                if mem[val] != 0:\n",
    "                    continue\n",
    "            else:\n",
    "                mem[val] = 0\n",
    "        nts = grammar.search_nts(cur, NONTERMS)\n",
    "        if len(nts) == 0:\n",
    "            if nx.is_isomorphic(cur, graph, node_match=node_match):\n",
    "                with lock: \n",
    "                    mem[val] = [[]]\n",
    "            else:\n",
    "                with lock:\n",
    "                    mem[val] = []\n",
    "            continue # done        \n",
    "        done = True\n",
    "        res = []\n",
    "        for j, nt in enumerate(nts):\n",
    "            for i, rule in enumerate(grammar.rules):                      \n",
    "                if rule is None:\n",
    "                    continue\n",
    "                nt_label = cur.nodes[nt]['label']\n",
    "                if rule.nt == nt_label:\n",
    "                    c = rule(cur, nt)\n",
    "                    if not nx.is_connected(nx.Graph(c)):\n",
    "                        continue\n",
    "                    if not nx.is_directed_acyclic_graph(c):\n",
    "                        continue\n",
    "                    exist = find_partial([graph], c)\n",
    "                    if not exist:\n",
    "                        continue\n",
    "                    hash_val = wl_hash(c)\n",
    "                    with lock:\n",
    "                        if hash_val not in mem:\n",
    "                            if done:\n",
    "                                stack.append((cur, val))\n",
    "                                done = False\n",
    "                            stack.append((c, hash_val))                            \n",
    "                        else:\n",
    "                            if mem[hash_val] == 0:\n",
    "                                if done:                                \n",
    "                                    stack.append((cur, val))                            \n",
    "                                    done = False\n",
    "                            else:\n",
    "                                for seq in mem[hash_val]: # res\n",
    "                                    res.append([i]+deepcopy(seq))\n",
    "        with lock:\n",
    "            if done:        \n",
    "                mem[val] = res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_to_key(S):\n",
    "    return tuple(sorted(list(S)))\n",
    "\n",
    "def hitting(elim_sets, beam_width=100):\n",
    "    H = []\n",
    "    heapq.heappush(H, (0, set())) # priority=-len\n",
    "    for S in elim_sets: # beam search\n",
    "        # priority queue\n",
    "        H_copy = list(H)\n",
    "        H = []\n",
    "        for _, h in H_copy: # copy\n",
    "            for s in S:\n",
    "                if len(H) < beam_width:\n",
    "                    h_ = h|set([s])\n",
    "                    heapq.heappush(H, (-len(h_), h_))\n",
    "                else:\n",
    "                    val_, h_ = heapq.heappop(H)\n",
    "                    h__ = h|set([s])\n",
    "                    if -val_ <= len(h_):\n",
    "                        heapq.heappush(H, (val_, h_))\n",
    "                    else:\n",
    "                        heapq.heappush(H, (-len(h__), h__))\n",
    "    while len(H)>1:\n",
    "        heapq.heappop(H)\n",
    "    l, ans = heapq.heappop(H)\n",
    "    print(f\"Input: {elim_sets}, Output: {ans}\")\n",
    "    return ans\n",
    "\n",
    "\n",
    "def try_disambiguate(derivs):\n",
    "    counts = {}\n",
    "    for deriv in derivs:\n",
    "        key = set_to_key(deriv)\n",
    "        if key not in counts:\n",
    "            counts[key] = []\n",
    "        counts[key].append(deriv)\n",
    "    stuff = hitting(counts.keys()) # test\n",
    "    elim_rule_sets = set()\n",
    "    for key in counts:\n",
    "        if len(counts[key]) == 1:\n",
    "            keep_deriv = counts[key][0]\n",
    "            elim_sets = set()\n",
    "            for deriv in derivs:\n",
    "                if deriv == keep_deriv:\n",
    "                    continue\n",
    "                elim_sets.add(set_to_key(set(deriv)-set(keep_deriv)))\n",
    "            elim_rule_set = hitting(elim_sets)\n",
    "            elim_rule_sets.add(set_to_key(elim_rule_set))\n",
    "    # elim rule set\n",
    "    min_num_remove = len(grammar.rules)\n",
    "    min_remove = None\n",
    "    for rule_set in elim_rule_sets:\n",
    "        num_remove = 0\n",
    "        for r in rule_set:\n",
    "            if grammar.rules[r] is not None:\n",
    "                num_remove += 1\n",
    "        if num_remove < min_num_remove:\n",
    "            min_num_remove = num_remove\n",
    "            min_remove = rule_set\n",
    "    if min_remove is None:\n",
    "        min_remove = hitting(counts.keys())\n",
    "    return min_remove\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = mp.Manager()\n",
    "stack = manager.list()\n",
    "found = manager.list()\n",
    "mem = manager.dict()\n",
    "lock = manager.Lock()\n",
    "sorted_graphs = sorted(graphs, key=len)\n",
    "for graph in sorted_graphs:\n",
    "    g = nx.DiGraph()\n",
    "    g.add_node('0', label='black')    \n",
    "    init_hash = wl_hash(g)\n",
    "    stack.append((deepcopy(g), init_hash))\n",
    "    processes = []\n",
    "    for _ in range(NUM_PROCS):\n",
    "        p = mp.Process(target=worker_single, args=(stack, grammar, graph, init_hash, mem, lock))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()    \n",
    "    derivs = mem[init_hash]\n",
    "    rule_set = try_disambiguate(derivs)\n",
    "    n = len(list(filter(None, grammar.rules)))\n",
    "    print(f\"removing rules {rule_set}: {n}->{n-len(rule_set)}\")\n",
    "    for r in rule_set:        \n",
    "        grammar.rules[r] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = set_to_key\n",
    "S = [sk({1,2,3}), sk({2,3,4,5}), sk({3,6,7}), sk({5,6,8}), sk({8,9,10})]\n",
    "hitting(set(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
