{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('/home/msun415/induction/')\n",
    "import pickle\n",
    "from src.config import METHOD, DATASET, GRAMMAR\n",
    "import importlib\n",
    "from src.examples import *\n",
    "from src.draw.color import to_hex, CMAP\n",
    "from src.draw.graph import draw_graph\n",
    "from src.config import RADIUS\n",
    "from argparse import ArgumentParser\n",
    "import pickle\n",
    "from src.grammar.common import get_args\n",
    "from src.grammar.ednce import *\n",
    "from src.draw.graph import *\n",
    "from src.api.get_motifs import *\n",
    "from src.algo.utils import *\n",
    "from src.algo.common import *\n",
    "from src.grammar.common import *\n",
    "from src.grammar.utils import *\n",
    "from src.algo.ednce import terminate, dfs\n",
    "from src.model import graph_regression, transformer_regression\n",
    "from argparse import ArgumentParser\n",
    "from networkx.algorithms.isomorphism import DiGraphMatcher\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    if DATASET == \"cora\":\n",
    "        g = load_cora()\n",
    "    elif DATASET == \"test\":\n",
    "        g = create_test_graph(1)\n",
    "    elif DATASET == \"debug\":\n",
    "        g = debug()\n",
    "    elif DATASET == \"house\":\n",
    "        g = create_house_graph()\n",
    "    elif DATASET == \"ckt\":\n",
    "        g = load_ckt(args)\n",
    "    elif DATASET == \"enas\":\n",
    "        g = load_enas(args)\n",
    "    elif DATASET == \"bn\":\n",
    "        g = load_bn(args)\n",
    "    elif DATASET == \"ast\":\n",
    "        g = load_ast(args)\n",
    "    elif DATASET == \"mol\":\n",
    "        g = load_mols(args)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return g\n",
    "    \n",
    "def get_args():\n",
    "    parser = ArgumentParser()\n",
    "    # global args\n",
    "    parser.add_argument(\"--visualize\", dest=\"global_visualize\", action='store_true')\n",
    "    parser.add_argument(\"--cache\", dest=\"global_cache\", action='store_true')    \n",
    "    parser.add_argument(\"--num_threads\", dest=\"global_num_threads\", type=int)\n",
    "    parser.add_argument(\"--num_procs\", dest=\"global_num_procs\", type=int)    \n",
    "    # hparams\n",
    "    parser.add_argument(\"--scheme\", choices=['one','zero'], help='whether to index from 0 or 1', default='zero')    \n",
    "    # ablations\n",
    "    parser.add_argument(\"--ablate_tree\", action='store_true') \n",
    "    parser.add_argument(\"--ablate_merge\", action='store_true') \n",
    "    parser.add_argument(\"--ablate_root\", action='store_true') \n",
    "    # task params\n",
    "    parser.add_argument(\"--task\", nargs='+', choices=[\"learn\",\"generate\",\"prediction\"])\n",
    "    parser.add_argument(\"--seed\")\n",
    "    parser.add_argument(\"--grammar_ckpt\")\n",
    "    # mol dataset args\n",
    "    parser.add_argument(\n",
    "        \"--mol-dataset\",\n",
    "        choices=[\"ptc\",\"hopv\",\"polymers_117\", \"isocyanates\", \"chain_extenders\", \"acrylates\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-data-samples\", type=int\n",
    "    )\n",
    "    parser.add_argument(\"--ambiguous-file\", help='if given and exists, load data from this file to learn grammar; if given and not exist, save ambiguous data to this file after learn grammar')\n",
    "    parser.add_argument(\"--num_samples\", default=10000, type=int, help='how much to generate')\n",
    "    return parser.parse_args([\n",
    "        '--task', 'learn',\n",
    "        '--ambiguous-file', 'cache/api_bn_ednce/ambig_1.json'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "g = load_data(args)\n",
    "cache_iter, cache_path = setup()\n",
    "g, grammar, anno, iter = init_grammar(g, cache_iter, cache_path, EDNCEGrammar)\n",
    "# grammar, model, anno, g = terminate(g, grammar, anno, iter)\n",
    "# for j, m in enumerate(model):\n",
    "#     pre = get_prefix(m.id)\n",
    "#     # draw_tree(m, os.path.join(IMG_DIR, f\"model_{iter}_{pre}.png\"))\n",
    "#     model[j] = EDNCEModel(dfs(anno, m.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "for j in range(1):\n",
    "    deriv = model[j].seq[::-1]\n",
    "    deriv = [model[j].graph[n].attrs['rule'] for n in deriv]\n",
    "    t2r = {i:i for i in range(len(grammar.rules))}\n",
    "    deriv_g = grammar.derive(deriv, t2r)\n",
    "    draw_graph(deriv_g, '/home/msun415/test.png')\n",
    "    graphs.append(deriv_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(graphs, query):\n",
    "    ans = []\n",
    "    for i in range(len(graphs)):\n",
    "        if nx.is_isomorphic(graphs[i], query):\n",
    "            ans.append(i)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def node_match(d1, d2):\n",
    "    return d1.get(\"label\", \"#\") == d2.get(\"label\", \"#\")    \n",
    "\n",
    "\n",
    "def find_partial(graphs, query):\n",
    "    ans = []\n",
    "    # query can be a (possibly disconnected) directed graph\n",
    "    # query_und = nx.Graph(query)\n",
    "    for i in range(len(graphs)):\n",
    "        bad = False\n",
    "        if len(query) > len(graphs[i]):\n",
    "            continue\n",
    "        if len(query.edges) > len(graphs[i].edges):\n",
    "            continue\n",
    "        # for conn in nx.connected_components(query_und):\n",
    "            # conn_g = copy_graph(query, conn)\n",
    "        gm = DiGraphMatcher(graphs[i], query, node_match=node_match)\n",
    "        ism_iter = list(gm.subgraph_isomorphisms_iter())\n",
    "        if len(ism_iter) == 0:\n",
    "            break\n",
    "        if not bad:\n",
    "            ans.append(i)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def worker(shared_queue, found, lock):\n",
    "    while True:\n",
    "        with lock:\n",
    "            if shared_queue.empty():\n",
    "                print(\"process done\")\n",
    "                break\n",
    "            print(f\"len(interms): {shared_queue.qsize()}\")\n",
    "            interm, deriv, poss = shared_queue.get()\n",
    "        print(f\"deriv: {deriv}\")\n",
    "        nts = grammar.search_nts(interm, NONTERMS)\n",
    "        if len(nts) == 0:\n",
    "            if nx.is_isomorphic(interm, graphs[poss], node_match=node_match):\n",
    "                with lock:\n",
    "                    found[poss].append(deriv)\n",
    "                    print(f\"found {deriv} graph {poss}, count: {len(found[poss])}\")\n",
    "        for j, nt in enumerate(nts):\n",
    "            for i, rule in enumerate(grammar.rules):                      \n",
    "                nt_label = interm.nodes[nt]['label']\n",
    "                if rule.nt == nt_label:\n",
    "                    c = deepcopy(interm)\n",
    "                    c = rule(c, nt)\n",
    "                    if nx.is_connected(nx.Graph(c)):\n",
    "                        # if poss == 0 and i == 62:\n",
    "                        #     pdb.set_trace()                     \n",
    "                        ts = [x for x in c if c.nodes[x]['label'] in TERMS]\n",
    "                        c_t = copy_graph(c, ts)\n",
    "                        exist = find_partial([graphs[poss]], c_t)\n",
    "                        if exist:\n",
    "                            with lock:\n",
    "                                shared_queue.put((c, deriv+[i], poss))            \n",
    "\n",
    "\n",
    "NUM_PROCS = 50\n",
    "N = len(graphs)\n",
    "manager = mp.Manager()\n",
    "shared_queue = manager.Queue()\n",
    "found = manager.list()\n",
    "g = nx.DiGraph()\n",
    "g.add_node('0', label='black')\n",
    "for j in range(NUM_PROCS):\n",
    "    shared_queue.put((deepcopy(g), [], j))\n",
    "    found.append(manager.list())\n",
    "lock = manager.Lock()\n",
    "processes = []\n",
    "for _ in range(NUM_PROCS):\n",
    "    p = mp.Process(target=worker, args=(shared_queue, found, lock))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_derivs = list(map(list, found))\n",
    "sets_of_sets = []\n",
    "for derivs in all_derivs:\n",
    "    sets = []\n",
    "    for i in range(len(derivs)):\n",
    "        # keep this deriv\n",
    "        for j in range(len(derivs)):\n",
    "            if j == i:\n",
    "                continue\n",
    "            sets.append(set(derivs[j])-set(derivs[i]))\n",
    "    sets_of_sets.append(sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere([len(deriv) > 1 for deriv in all_derivs])\n",
    "# [len(deriv) > 1 for deriv in all_derivs]\n",
    "poss_elims = []\n",
    "for chosen in product(*[sets for sets in sets_of_sets if sets]):\n",
    "    elim = set.union(*chosen)\n",
    "    exist = False\n",
    "    for p in poss_elims:\n",
    "        if p == elim:\n",
    "            exist = True\n",
    "            break\n",
    "    if not exist:\n",
    "        poss_elims.append(elim)\n",
    "\n",
    "poss_elims = sorted(poss_elims, key=len)\n",
    "for i in range(len(poss_elims)):\n",
    "    if poss_elims[i] is None:\n",
    "        continue\n",
    "    for j in range(i+1, len(poss_elims)):\n",
    "        if poss_elims[j] is None:\n",
    "            continue\n",
    "        if not (poss_elims[i]-poss_elims[j]):\n",
    "            poss_elims[j] = None\n",
    "\n",
    "min_poss_elims = list(filter(lambda x: x is not None, poss_elims))\n",
    "best_e = None\n",
    "best_counter = None\n",
    "for e in min_poss_elims:\n",
    "    counter = []\n",
    "    for i, derivs in enumerate(all_derivs):\n",
    "        inters = [bool(set(derivs[j]) & e) for j in range(len(derivs))]\n",
    "        if np.all(inters):\n",
    "            counter.append(i)\n",
    "    if best_counter is None or len(counter) < len(best_counter):\n",
    "        best_counter = counter\n",
    "        best_e = e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmap = {}\n",
    "d = 0\n",
    "for i in range(len(grammar.rules)):\n",
    "    if i in best_e:\n",
    "        d += 1\n",
    "        continue\n",
    "    rmap[i] = i-d\n",
    "\n",
    "for i in range(len(model)-1,-1,-1):\n",
    "    if i in best_counter:\n",
    "        model.pop(i)\n",
    "        continue\n",
    "    for n in model[i].graph:\n",
    "        r = model[i].graph[n].attrs['rule']\n",
    "        model[i].graph[n].attrs['rule'] = rmap[r]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
